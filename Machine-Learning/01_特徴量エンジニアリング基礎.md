# 特徴量エンジニアリング基礎 - 機械学習の成功を決める重要な技術

## 1. 特徴量エンジニアリングとは？

### 1.1 特徴量エンジニアリングの重要性

特徴量エンジニアリングは、機械学習において最も重要な技術の一つです。簡単に言うと、**「生のデータを機械学習アルゴリズムが理解しやすい形に変換する作業」**です。

**なぜ重要なのでしょうか？**

- 機械学習モデルの性能は、データの質に大きく依存します
- 良い特徴量を作ることで、シンプルなモデルでも高い性能を達成できます
- 逆に、特徴量が悪いと、複雑なモデルでも良い結果が得られません

**例：住宅価格予測**
```
生データ：
- 住所: "東京都渋谷区神南1-2-3"
- 築年数: 15年
- 面積: 80㎡
- 駅からの距離: 500m

特徴量エンジニアリング後：
- 都道府県: 東京都
- 区: 渋谷区
- 築年数: 15
- 面積: 80
- 駅からの距離: 500
- 築年数カテゴリ: "10-20年"
- 面積カテゴリ: "70-90㎡"
- 利便性スコア: 85点
```

## 2. テキスト特徴量抽出

### 2.1 TF-IDF（Term Frequency-Inverse Document Frequency）

**TF-IDFとは？**
テキストデータを数値化する最も基本的な手法です。単語の重要度を計算します。

**計算方法：**
1. **TF（Term Frequency）**: 文書内での単語の出現頻度
2. **IDF（Inverse Document Frequency）**: 全文書での単語の希少性

**数式：**
```
TF-IDF = TF × IDF
TF = (単語の出現回数) / (文書内の総単語数)
IDF = log(総文書数 / その単語を含む文書数)
```

**Python実装例：**
```python
from sklearn.feature_extraction.text import TfidfVectorizer

# サンプルテキスト
texts = [
    "Pythonは素晴らしいプログラミング言語です",
    "機械学習でPythonを使います",
    "データサイエンスとPythonは相性が良い"
]

# TF-IDFベクトライザー
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(texts)

print("特徴量名:", vectorizer.get_feature_names_out())
print("TF-IDF行列:\n", tfidf_matrix.toarray())
```

### 2.2 Word2Vec

**Word2Vecとは？**
単語をベクトル（数値の配列）に変換する手法です。似た意味の単語は似たベクトルになります。

**仕組み：**
- 単語の周辺にある単語から、その単語の意味を学習
- 高次元のベクトル空間で単語を表現
- 意味的に近い単語は近い位置に配置される

**Python実装例：**
```python
import numpy as np
from gensim.models import Word2Vec

# サンプルテキスト（単語に分割済み）
sentences = [
    ['Python', 'は', '素晴らしい', 'プログラミング', '言語', 'です'],
    ['機械学習', 'で', 'Python', 'を', '使います'],
    ['データ', 'サイエンス', 'と', 'Python', 'は', '相性', 'が', '良い']
]

# Word2Vecモデルの学習
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# 単語のベクトル取得
python_vector = model.wv['Python']
print("Pythonのベクトル（最初の10次元）:", python_vector[:10])

# 類似単語の検索
similar_words = model.wv.most_similar('Python')
print("Pythonに類似した単語:", similar_words)
```

### 2.3 BERT埋め込み

**BERTとは？**
Googleが開発した最新の言語モデルです。文脈を理解して単語や文をベクトル化します。

**特徴：**
- 文脈を考慮した単語の意味理解
- 双方向のTransformerアーキテクチャ
- 事前学習済みモデルの利用

**Python実装例：**
```python
from transformers import BertTokenizer, BertModel
import torch

# BERTモデルとトークナイザーの読み込み
tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')
model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese')

# テキストのベクトル化
text = "Pythonで機械学習を学ぶ"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

# 文の埋め込みベクトル（[CLS]トークンの出力）
sentence_embedding = outputs.last_hidden_state[:, 0, :]
print("文のベクトル次元:", sentence_embedding.shape)
```

## 3. 時系列特徴量

### 3.1 ラグ特徴量

**ラグ特徴量とは？**
過去の値を使って新しい特徴量を作る手法です。時系列データの予測に非常に重要です。

**例：売上予測**
```
元データ：
日付: 2024-01-01, 売上: 100万円
日付: 2024-01-02, 売上: 120万円
日付: 2024-01-03, 売上: 110万円

ラグ特徴量追加後：
日付: 2024-01-03, 売上: 110万円, 売上_1日前: 120万円, 売上_2日前: 100万円
```

**Python実装例：**
```python
import pandas as pd

# サンプル時系列データ
df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=10),
    'sales': [100, 120, 110, 130, 125, 140, 135, 150, 145, 160]
})

# ラグ特徴量の作成
df['sales_lag1'] = df['sales'].shift(1)  # 1日前
df['sales_lag2'] = df['sales'].shift(2)  # 2日前
df['sales_lag7'] = df['sales'].shift(7)  # 1週間前

print("ラグ特徴量追加後:")
print(df)
```

### 3.2 移動平均

**移動平均とは？**
一定期間の平均値を計算して、トレンドや季節性を捉える手法です。

**種類：**
- **単純移動平均（SMA）**: 単純に平均を取る
- **指数移動平均（EMA）**: 最近のデータにより重みを置く

**Python実装例：**
```python
import pandas as pd
import numpy as np

# サンプルデータ
df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=30),
    'sales': np.random.normal(100, 20, 30)  # 平均100、標準偏差20の乱数
})

# 移動平均の計算
df['sma_7'] = df['sales'].rolling(window=7).mean()  # 7日移動平均
df['ema_7'] = df['sales'].ewm(span=7).mean()        # 7日指数移動平均

print("移動平均計算後（最初の10行）:")
print(df.head(10))
```

### 3.3 季節性分解

**季節性分解とは？**
時系列データを以下の成分に分解します：
- **トレンド**: 長期的な変化
- **季節性**: 周期的なパターン
- **残差**: ランダムな変動

**Python実装例：**
```python
from statsmodels.tsa.seasonal import seasonal_decompose
import matplotlib.pyplot as plt

# 季節性のあるデータを作成
dates = pd.date_range('2020-01-01', periods=365*2, freq='D')
trend = np.linspace(100, 120, len(dates))
seasonal = 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365)
noise = np.random.normal(0, 5, len(dates))
data = trend + seasonal + noise

# 季節性分解
decomposition = seasonal_decompose(data, period=365)

# 結果の可視化
fig, axes = plt.subplots(4, 1, figsize=(12, 8))
decomposition.observed.plot(ax=axes[0], title='元データ')
decomposition.trend.plot(ax=axes[1], title='トレンド')
decomposition.seasonal.plot(ax=axes[2], title='季節性')
decomposition.resid.plot(ax=axes[3], title='残差')
plt.tight_layout()
plt.show()
```

## 4. カテゴリカル特徴量

### 4.1 One-Hot Encoding

**One-Hot Encodingとは？**
カテゴリカル変数を0と1のベクトルに変換する手法です。

**例：都道府県データ**
```
元データ：
都道府県: ["東京", "大阪", "東京", "福岡"]

One-Hot Encoding後：
東京: [1, 0, 0, 0]
大阪: [0, 1, 0, 0]
福岡: [0, 0, 0, 1]
```

**Python実装例：**
```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# サンプルデータ
df = pd.DataFrame({
    'prefecture': ['東京', '大阪', '東京', '福岡', '大阪'],
    'age': [25, 30, 35, 28, 32]
})

# One-Hot Encoding
encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform(df[['prefecture']])

# 結果をDataFrameに変換
encoded_df = pd.DataFrame(
    encoded, 
    columns=encoder.get_feature_names_out(['prefecture'])
)

# 元データと結合
result = pd.concat([df, encoded_df], axis=1)
print("One-Hot Encoding結果:")
print(result)
```

### 4.2 Label Encoding

**Label Encodingとは？**
カテゴリカル変数を数値に変換する手法です。順序がある場合に適しています。

**例：教育レベル**
```
元データ：
教育レベル: ["高校", "大学", "大学院", "高校", "大学"]

Label Encoding後：
高校: 0
大学: 1
大学院: 2
```

**Python実装例：**
```python
from sklearn.preprocessing import LabelEncoder

# サンプルデータ
education = ['高校', '大学', '大学院', '高校', '大学']

# Label Encoding
encoder = LabelEncoder()
encoded = encoder.fit_transform(education)

print("元データ:", education)
print("Label Encoding後:", encoded)
print("クラス名:", encoder.classes_)
```

### 4.3 Target Encoding

**Target Encodingとは？**
カテゴリカル変数を、そのカテゴリの目的変数の平均値で置き換える手法です。

**例：都道府県別の購買率**
```
元データ：
都道府県: ["東京", "大阪", "東京", "福岡"]
購買: [1, 0, 1, 0]

Target Encoding後：
東京: 1.0 (2回中2回購買)
大阪: 0.0 (1回中0回購買)
福岡: 0.0 (1回中0回購買)
```

**Python実装例：**
```python
import pandas as pd

# サンプルデータ
df = pd.DataFrame({
    'prefecture': ['東京', '大阪', '東京', '福岡', '大阪', '東京'],
    'purchase': [1, 0, 1, 0, 1, 0]
})

# Target Encoding
target_encoding = df.groupby('prefecture')['purchase'].mean()
print("Target Encoding結果:")
print(target_encoding)

# 新しいデータに適用
df['prefecture_encoded'] = df['prefecture'].map(target_encoding)
print("\n適用結果:")
print(df)
```

## 5. 数値特徴量

### 5.1 正規化（Normalization）

**正規化とは？**
データを0から1の範囲に変換する手法です。

**数式：**
```
x_normalized = (x - x_min) / (x_max - x_min)
```

**Python実装例：**
```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# サンプルデータ
data = np.array([[100, 25], [200, 30], [150, 28], [300, 35]])

# 正規化
scaler = MinMaxScaler()
normalized_data = scaler.fit_transform(data)

print("元データ:")
print(data)
print("\n正規化後:")
print(normalized_data)
```

### 5.2 標準化（Standardization）

**標準化とは？**
データを平均0、標準偏差1に変換する手法です。

**数式：**
```
x_standardized = (x - μ) / σ
```

**Python実装例：**
```python
from sklearn.preprocessing import StandardScaler

# 標準化
scaler = StandardScaler()
standardized_data = scaler.fit_transform(data)

print("標準化後:")
print(standardized_data)
print(f"平均: {standardized_data.mean(axis=0)}")
print(f"標準偏差: {standardized_data.std(axis=0)}")
```

### 5.3 対数変換

**対数変換とは？**
データの分布を正規分布に近づける手法です。右に歪んだデータに効果的です。

**Python実装例：**
```python
import numpy as np
import matplotlib.pyplot as plt

# 右に歪んだデータを作成
skewed_data = np.random.exponential(scale=2, size=1000)

# 対数変換
log_data = np.log1p(skewed_data)  # log1pはlog(1+x)を計算

# 分布の比較
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
ax1.hist(skewed_data, bins=50, alpha=0.7)
ax1.set_title('元データの分布')
ax2.hist(log_data, bins=50, alpha=0.7)
ax2.set_title('対数変換後の分布')
plt.show()
```

### 5.4 多項式特徴量

**多項式特徴量とは？**
既存の特徴量から2次、3次の項を作成する手法です。非線形な関係を捉えるのに役立ちます。

**Python実装例：**
```python
from sklearn.preprocessing import PolynomialFeatures

# サンプルデータ
X = np.array([[1, 2], [3, 4], [5, 6]])

# 2次多項式特徴量の作成
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

print("元データ:")
print(X)
print("\n多項式特徴量（2次）:")
print(X_poly)
print("\n特徴量名:", poly.get_feature_names_out(['x1', 'x2']))
```

## 6. 地理的特徴量

### 6.1 座標変換

**座標変換とは？**
緯度・経度を他の座標系に変換する手法です。

**Python実装例：**
```python
import pandas as pd
import numpy as np

# サンプル座標データ
df = pd.DataFrame({
    'latitude': [35.6762, 34.6937, 35.1815],  # 東京、大阪、名古屋
    'longitude': [139.6503, 135.5023, 136.9066]
})

# 度からラジアンへの変換
df['lat_rad'] = np.radians(df['latitude'])
df['lon_rad'] = np.radians(df['longitude'])

print("座標変換結果:")
print(df)
```

### 6.2 距離計算

**距離計算とは？**
2点間の距離を計算する手法です。

**Python実装例：**
```python
from sklearn.metrics.pairwise import haversine_distances

# 東京と大阪の座標
tokyo = [35.6762, 139.6503]  # 緯度、経度
osaka = [34.6937, 135.5023]

# ハーバサイン距離の計算
tokyo_rad = np.radians(tokyo)
osaka_rad = np.radians(osaka)

distance = haversine_distances([tokyo_rad, osaka_rad])[0, 1] * 6371  # km
print(f"東京と大阪の距離: {distance:.1f}km")
```

### 6.3 地域クラスタリング

**地域クラスタリングとは？**
地理的に近い地域をグループ化する手法です。

**Python実装例：**
```python
from sklearn.cluster import KMeans

# 日本の主要都市の座標
cities = pd.DataFrame({
    'city': ['東京', '大阪', '名古屋', '福岡', '札幌', '仙台'],
    'latitude': [35.6762, 34.6937, 35.1815, 33.5902, 43.0618, 38.2688],
    'longitude': [139.6503, 135.5023, 136.9066, 130.4017, 141.3545, 140.8721]
})

# K-meansクラスタリング
kmeans = KMeans(n_clusters=3, random_state=42)
cities['cluster'] = kmeans.fit_predict(cities[['latitude', 'longitude']])

print("地域クラスタリング結果:")
print(cities)
```

## 7. 実践的な特徴量エンジニアリングの流れ

### 7.1 基本的な流れ

1. **データの理解**
   - データ型の確認
   - 欠損値の確認
   - 外れ値の確認

2. **前処理**
   - 欠損値の処理
   - 外れ値の処理
   - データ型の変換

3. **特徴量作成**
   - ドメイン知識を活用
   - 基本的な統計量
   - 時系列特徴量

4. **特徴量選択**
   - 相関分析
   - 統計的検定
   - 機械学習による選択

**Python実装例：**
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression

# サンプルデータ
df = pd.DataFrame({
    'age': [25, 30, 35, 28, 32, 40],
    'income': [300, 400, 500, 350, 450, 600],
    'education': ['大学', '高校', '大学院', '大学', '高校', '大学'],
    'city': ['東京', '大阪', '東京', '福岡', '大阪', '東京'],
    'target': [1, 0, 1, 0, 1, 1]
})

# 1. データの理解
print("データ型:")
print(df.dtypes)
print("\n欠損値:")
print(df.isnull().sum())

# 2. 前処理
# カテゴリカル変数のエンコーディング
df_encoded = pd.get_dummies(df, columns=['education', 'city'])

# 3. 特徴量作成
df_encoded['age_squared'] = df_encoded['age'] ** 2
df_encoded['income_per_age'] = df_encoded['income'] / df_encoded['age']

# 4. 特徴量選択
X = df_encoded.drop('target', axis=1)
y = df_encoded['target']

# 相関分析
correlation = X.corrwith(y).abs().sort_values(ascending=False)
print("\n目的変数との相関:")
print(correlation)

# 統計的検定による特徴量選択
selector = SelectKBest(score_func=f_regression, k=5)
X_selected = selector.fit_transform(X, y)
selected_features = X.columns[selector.get_support()]
print("\n選択された特徴量:")
print(selected_features)
```

## 8. まとめ

特徴量エンジニアリングは機械学習の成功を左右する重要な技術です。以下のポイントを押さえておきましょう：

### 8.1 重要なポイント

1. **データの理解**: まずはデータをよく観察する
2. **ドメイン知識**: 業務知識を活用した特徴量作成
3. **段階的アプローチ**: 簡単な特徴量から始めて徐々に複雑に
4. **検証**: 作成した特徴量が本当に有用か検証する

### 8.2 よくある失敗

1. **過度な特徴量作成**: 過学習の原因になる
2. **リーク**: 未来の情報を使ってしまう
3. **スケールの不統一**: 特徴量間でスケールが異なる
4. **検証不足**: 作成した特徴量の効果を検証しない

### 8.3 次のステップ

1. **基本的な特徴量エンジニアリング**をマスター
2. **ドメイン特化の特徴量**を学ぶ
3. **自動特徴量エンジニアリング**ツールを活用
4. **特徴量選択**の手法を習得

特徴量エンジニアリングは経験と試行錯誤が重要な技術です。多くのデータセットで練習を重ねることで、自然と良い特徴量を作成できるようになります。 