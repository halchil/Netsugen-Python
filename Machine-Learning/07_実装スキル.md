# 実装スキル - 機械学習システムを構築する技術

## 1. 分散処理

### 1.1 分散処理とは？

**分散処理とは？**
複数のコンピュータを協調させて大規模なデータ処理を行う技術です。

**なぜ必要なのでしょうか？**

1. **データサイズの増大**: 単一マシンでは処理できない大規模データ
2. **処理時間の短縮**: 並列処理による高速化
3. **スケーラビリティ**: リソースの柔軟な拡張
4. **耐障害性**: 一部の故障に対する耐性

**例：1TBのデータ処理**
```
単一マシン（16GB RAM）：
- データを分割して処理
- 処理時間：数時間〜数日
- メモリ不足のリスク

分散処理（10台のマシン、各16GB RAM）：
- データを並列処理
- 処理時間：数分〜数時間
- 効率的なメモリ使用
```

### 1.2 PySpark

**PySparkとは？**
Apache SparkのPython APIです。大規模データの分散処理を実行できます。

**Python実装例：**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, sum
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import pandas as pd
import numpy as np

# Sparkセッションの作成
spark = SparkSession.builder \
    .appName("DistributedMachineLearning") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

# 大規模データの生成
np.random.seed(42)
n_samples = 100000

# サンプルデータ
data = {
    'user_id': np.random.randint(1, 10000, n_samples),
    'age': np.random.normal(35, 10, n_samples),
    'income': np.random.normal(500, 150, n_samples),
    'purchase_amount': np.random.exponential(100, n_samples),
    'purchase_count': np.random.poisson(5, n_samples),
    'region': np.random.choice(['Tokyo', 'Osaka', 'Nagoya', 'Fukuoka'], n_samples)
}

# Pandas DataFrameからSpark DataFrameへの変換
df_pandas = pd.DataFrame(data)
df_spark = spark.createDataFrame(df_pandas)

print("Spark DataFrameの基本情報:")
print(f"データ数: {df_spark.count()}")
print(f"カラム数: {len(df_spark.columns)}")
df_spark.printSchema()

# 分散データ処理の例
print("\n分散データ処理の例:")

# 1. 基本的な集計
summary_stats = df_spark.groupBy('region').agg(
    count('*').alias('user_count'),
    avg('age').alias('avg_age'),
    avg('income').alias('avg_income'),
    sum('purchase_amount').alias('total_purchase')
)

print("地域別統計:")
summary_stats.show()

# 2. 複雑な集計
age_income_stats = df_spark.groupBy('region').agg(
    avg('age').alias('avg_age'),
    avg('income').alias('avg_income'),
    avg('purchase_amount').alias('avg_purchase'),
    count('*').alias('user_count')
).filter(col('user_count') > 100)

print("地域別詳細統計（ユーザー数100人以上）:")
age_income_stats.show()

# 3. 分散機械学習
# 特徴量の準備
feature_cols = ['age', 'income', 'purchase_count']
assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol='features'
)

# 目的変数の作成（購買金額が平均以上かどうか）
avg_purchase = df_spark.agg(avg('purchase_amount')).collect()[0][0]
df_with_target = df_spark.withColumn(
    'high_purchase', 
    (col('purchase_amount') > avg_purchase).cast('int')
)

# データの分割
train_data, test_data = df_with_target.randomSplit([0.8, 0.2], seed=42)

# Random Forestモデルの学習
rf = RandomForestClassifier(
    labelCol='high_purchase',
    featuresCol='features',
    numTrees=100,
    maxDepth=10,
    seed=42
)

# パイプラインの構築
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=[assembler, rf])

# モデルの学習
model = pipeline.fit(train_data)

# 予測
predictions = model.transform(test_data)

# 評価
evaluator = MulticlassClassificationEvaluator(
    labelCol='high_purchase',
    predictionCol='prediction',
    metricName='accuracy'
)

accuracy = evaluator.evaluate(predictions)
print(f"分散Random Forestの正解率: {accuracy:.3f}")

# 特徴量の重要度
rf_model = model.stages[1]
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': rf_model.featureImportances.toArray()
}).sort_values('importance', ascending=False)

print("\n特徴量の重要度:")
print(feature_importance)

# Sparkセッションの終了
spark.stop()
```

### 1.3 Dask

**Daskとは？**
PandasやNumPyの分散版です。既存のコードを最小限の変更で分散処理できます。

**Python実装例：**
```python
import dask.dataframe as dd
import dask.array as da
from dask.distributed import Client
import numpy as np
import pandas as pd

# Daskクライアントの作成
client = Client(n_workers=4, threads_per_worker=2)
print(f"Daskダッシュボード: {client.dashboard_link}")

# 大規模データの作成
np.random.seed(42)
n_samples = 1000000  # 100万サンプル

# サンプルデータ
data = {
    'user_id': np.random.randint(1, 100000, n_samples),
    'age': np.random.normal(35, 10, n_samples),
    'income': np.random.normal(500, 150, n_samples),
    'purchase_amount': np.random.exponential(100, n_samples),
    'purchase_count': np.random.poisson(5, n_samples),
    'region': np.random.choice(['Tokyo', 'Osaka', 'Nagoya', 'Fukuoka'], n_samples)
}

# Pandas DataFrameからDask DataFrameへの変換
df_pandas = pd.DataFrame(data)
df_dask = dd.from_pandas(df_pandas, npartitions=4)

print("Dask DataFrameの基本情報:")
print(f"パーティション数: {df_dask.npartitions}")
print(f"データ形状: {df_dask.shape}")
print(f"カラム: {list(df_dask.columns)}")

# 分散データ処理の例
print("\n分散データ処理の例:")

# 1. 基本的な集計
summary_stats = df_dask.groupby('region').agg({
    'user_id': 'count',
    'age': 'mean',
    'income': 'mean',
    'purchase_amount': 'sum'
}).compute()

print("地域別統計:")
print(summary_stats)

# 2. 複雑な集計
age_income_stats = df_dask.groupby('region').agg({
    'age': ['mean', 'std'],
    'income': ['mean', 'std'],
    'purchase_amount': ['mean', 'sum']
}).compute()

print("\n地域別詳細統計:")
print(age_income_stats)

# 3. 分散機械学習
from dask_ml.model_selection import train_test_split
from dask_ml.ensemble import RandomForestClassifier
from dask_ml.metrics import accuracy_score

# 特徴量の準備
feature_cols = ['age', 'income', 'purchase_count']
X = df_dask[feature_cols]
y = (df_dask['purchase_amount'] > df_dask['purchase_amount'].mean()).astype(int)

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forestモデルの学習
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 予測
y_pred = rf.predict(X_test)

# 評価
accuracy = accuracy_score(y_test, y_pred)
print(f"分散Random Forestの正解率: {accuracy:.3f}")

# 特徴量の重要度
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("\n特徴量の重要度:")
print(feature_importance)

# クライアントの終了
client.close()
```

### 1.4 Ray

**Rayとは？**
分散機械学習と強化学習に特化したフレームワークです。

**Python実装例：**
```python
import ray
from ray import tune
from ray.tune.schedulers import ASHAScheduler
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Rayの初期化
ray.init()

# サンプルデータの作成
np.random.seed(42)
n_samples = 10000

X = np.random.randn(n_samples, 10)
y = (np.sum(X, axis=1) > 0).astype(int)

# データ分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 分散ハイパーパラメータチューニング
def train_model(config):
    """分散学習関数"""
    model = RandomForestClassifier(
        n_estimators=config['n_estimators'],
        max_depth=config['max_depth'],
        random_state=42
    )
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    return {'accuracy': accuracy}

# ハイパーパラメータの探索空間
config = {
    'n_estimators': tune.choice([50, 100, 200]),
    'max_depth': tune.choice([5, 10, 15])
}

# 分散ハイパーパラメータチューニングの実行
scheduler = ASHAScheduler(
    metric='accuracy',
    mode='max',
    max_t=100,
    grace_period=10,
    reduction_factor=2
)

analysis = tune.run(
    train_model,
    config=config,
    num_samples=20,
    scheduler=scheduler,
    resources_per_trial={'cpu': 1}
)

print("最良の結果:")
print(analysis.best_config)
print(f"最良の精度: {analysis.best_result['accuracy']:.3f}")

# 結果の可視化
import matplotlib.pyplot as plt

# 結果の取得
results = analysis.results_df

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.scatter(results['config/n_estimators'], results['accuracy'])
plt.xlabel('n_estimators')
plt.ylabel('Accuracy')
plt.title('n_estimators vs Accuracy')

plt.subplot(1, 2, 2)
plt.scatter(results['config/max_depth'], results['accuracy'])
plt.xlabel('max_depth')
plt.ylabel('Accuracy')
plt.title('max_depth vs Accuracy')

plt.tight_layout()
plt.show()

# Rayの終了
ray.shutdown()
```

## 2. クラウドML

### 2.1 AWS SageMaker

**AWS SageMakerとは？**
Amazon Web Servicesの機械学習プラットフォームです。

**Python実装例：**
```python
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn import SKLearn
from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter
import pandas as pd
import numpy as np

# SageMakerセッションの作成
sagemaker_session = sagemaker.Session()
role = get_execution_role()

print(f"SageMaker実行ロール: {role}")

# サンプルデータの作成
np.random.seed(42)
n_samples = 10000

X = np.random.randn(n_samples, 10)
y = (np.sum(X, axis=1) > 0).astype(int)

# データの保存
data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])
data['target'] = y

# S3へのアップロード
data.to_csv('train_data.csv', index=False)
s3_bucket = 'your-bucket-name'
s3_prefix = 'ml-training'

# データのS3アップロード
s3_input_train = sagemaker_session.upload_data(
    path='train_data.csv',
    bucket=s3_bucket,
    key_prefix=f'{s3_prefix}/train'
)

print(f"トレーニングデータのS3パス: {s3_input_train}")

# SKLearnエスティメーターの作成
sklearn_estimator = SKLearn(
    entry_point='train.py',
    role=role,
    instance_count=1,
    instance_type='ml.m5.large',
    framework_version='0.23-1',
    py_version='py3',
    sagemaker_session=sagemaker_session
)

# ハイパーパラメータチューニングの設定
hyperparameter_ranges = {
    'n_estimators': IntegerParameter(50, 200),
    'max_depth': IntegerParameter(5, 15)
}

# ハイパーパラメータチューニングの実行
tuner = HyperparameterTuner(
    sklearn_estimator,
    'accuracy',
    hyperparameter_ranges,
    objective_metric_name='accuracy',
    max_jobs=10,
    max_parallel_jobs=2
)

# トレーニングの開始
tuner.fit({'train': s3_input_train})

print("ハイパーパラメータチューニング完了")
print(f"最良のジョブ: {tuner.best_training_job()}")

# 最良モデルのデプロイ
predictor = tuner.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.large'
)

# 予測の実行
test_data = np.random.randn(100, 10)
predictions = predictor.predict(test_data)

print(f"予測結果の形状: {predictions.shape}")

# エンドポイントの削除
predictor.delete_endpoint()
```

### 2.2 Google Vertex AI

**Google Vertex AIとは？**
Google Cloudの機械学習プラットフォームです。

**Python実装例：**
```python
from google.cloud import aiplatform
from google.cloud.aiplatform import gapis
import pandas as pd
import numpy as np

# Vertex AIの初期化
aiplatform.init(project='your-project-id', location='us-central1')

# サンプルデータの作成
np.random.seed(42)
n_samples = 10000

X = np.random.randn(n_samples, 10)
y = (np.sum(X, axis=1) > 0).astype(int)

# データの準備
data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])
data['target'] = y

# AutoMLの実行
dataset = aiplatform.TabularDataset.create(
    display_name='sample_dataset',
    gcs_source='gs://your-bucket/data.csv'
)

job = aiplatform.AutoMLTabularTrainingJob(
    display_name='sample_automl_job',
    optimization_prediction_type='classification',
    optimization_objective='maximize-au-prc'
)

model = job.run(
    dataset=dataset,
    target_column='target',
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    budget_milli_node_hours=1000
)

print("AutoMLトレーニング完了")
print(f"モデル名: {model.name}")

# モデルのデプロイ
endpoint = model.deploy(
    machine_type='n1-standard-4',
    accelerator_type='NVIDIA_TESLA_T4',
    accelerator_count=1
)

# 予測の実行
test_data = np.random.randn(100, 10)
predictions = endpoint.predict(test_data)

print(f"予測結果の形状: {predictions.shape}")

# エンドポイントの削除
endpoint.delete()
```

### 2.3 Azure ML

**Azure MLとは？**
Microsoft Azureの機械学習プラットフォームです。

**Python実装例：**
```python
from azureml.core import Workspace, Experiment, Environment
from azureml.core.compute import ComputeTarget
from azureml.train.sklearn import SKLearn
from azureml.core.dataset import Dataset
import pandas as pd
import numpy as np

# Azure MLワークスペースの作成
ws = Workspace.from_config()

# サンプルデータの作成
np.random.seed(42)
n_samples = 10000

X = np.random.randn(n_samples, 10)
y = (np.sum(X, axis=1) > 0).astype(int)

# データセットの作成
data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])
data['target'] = y

dataset = Dataset.Tabular.from_pandas_dataframe(
    data, 
    target='target'
)

# コンピューティングターゲットの取得
compute_target = ws.compute_targets['your-compute-cluster']

# 環境の作成
env = Environment.from_conda_specification(
    name='sklearn_env',
    file_path='conda_dependencies.yml'
)

# トレーニングスクリプトの設定
script_params = {
    '--n_estimators': 100,
    '--max_depth': 10
}

# SKLearnエスティメーターの作成
estimator = SKLearn(
    source_directory='.',
    entry_script='train.py',
    compute_target=compute_target,
    environment=env,
    hyperparameters=script_params
)

# 実験の作成と実行
experiment = Experiment(workspace=ws, name='sample-experiment')
run = experiment.submit(estimator)

# 実行の監視
run.wait_for_completion(show_output=True)

print("トレーニング完了")
print(f"実行ID: {run.id}")

# モデルの登録
model = run.register_model(
    model_name='sample_model',
    model_path='outputs'
)

print(f"モデル名: {model.name}")
```

## 3. パフォーマンス最適化

### 3.1 メモリ最適化

**メモリ最適化とは？**
大規模データセットを効率的に処理するためのメモリ使用量の最適化です。

**Python実装例：**
```python
import pandas as pd
import numpy as np
import psutil
import gc
from memory_profiler import profile

# メモリ使用量の監視
def print_memory_usage():
    """現在のメモリ使用量を表示"""
    process = psutil.Process()
    memory_info = process.memory_info()
    print(f"メモリ使用量: {memory_info.rss / 1024 / 1024:.2f} MB")

print("初期メモリ使用量:")
print_memory_usage()

# 大規模データセットの作成
np.random.seed(42)
n_samples = 1000000  # 100万サンプル

# 非最適化データの作成
data_non_optimized = {
    'user_id': np.random.randint(1, 1000000, n_samples),
    'age': np.random.normal(35, 10, n_samples),
    'income': np.random.normal(500, 150, n_samples),
    'category': np.random.choice(['A', 'B', 'C', 'D'], n_samples),
    'score': np.random.uniform(0, 100, n_samples)
}

df_non_optimized = pd.DataFrame(data_non_optimized)

print("\n非最適化データのメモリ使用量:")
print_memory_usage()
print(f"データ形状: {df_non_optimized.shape}")
print(f"メモリ使用量: {df_non_optimized.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

# データ型の最適化
def optimize_dtypes(df):
    """データ型を最適化してメモリ使用量を削減"""
    optimized_df = df.copy()
    
    # 数値型の最適化
    for col in optimized_df.select_dtypes(include=['int64']).columns:
        col_min = optimized_df[col].min()
        col_max = optimized_df[col].max()
        
        if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:
            optimized_df[col] = optimized_df[col].astype(np.int8)
        elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:
            optimized_df[col] = optimized_df[col].astype(np.int16)
        elif col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:
            optimized_df[col] = optimized_df[col].astype(np.int32)
    
    # 浮動小数点型の最適化
    for col in optimized_df.select_dtypes(include=['float64']).columns:
        optimized_df[col] = optimized_df[col].astype(np.float32)
    
    # カテゴリカル型の最適化
    for col in optimized_df.select_dtypes(include=['object']).columns:
        if optimized_df[col].nunique() / len(optimized_df) < 0.5:
            optimized_df[col] = optimized_df[col].astype('category')
    
    return optimized_df

# データ型の最適化
df_optimized = optimize_dtypes(df_non_optimized)

print("\n最適化後のメモリ使用量:")
print_memory_usage()
print(f"データ形状: {df_optimized.shape}")
print(f"メモリ使用量: {df_optimized.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

# メモリ削減率の計算
original_memory = df_non_optimized.memory_usage(deep=True).sum()
optimized_memory = df_optimized.memory_usage(deep=True).sum()
reduction_rate = (original_memory - optimized_memory) / original_memory * 100

print(f"メモリ削減率: {reduction_rate:.1f}%")

# データ型の比較
print("\nデータ型の比較:")
print("非最適化:")
print(df_non_optimized.dtypes)
print("\n最適化後:")
print(df_optimized.dtypes)

# チャンク処理によるメモリ最適化
def process_in_chunks(df, chunk_size=10000):
    """データをチャンクに分けて処理"""
    results = []
    
    for i in range(0, len(df), chunk_size):
        chunk = df.iloc[i:i+chunk_size]
        
        # チャンクでの処理
        chunk_result = chunk.groupby('category').agg({
            'age': 'mean',
            'income': 'mean',
            'score': 'mean'
        })
        
        results.append(chunk_result)
        
        # メモリの解放
        del chunk
        gc.collect()
    
    return pd.concat(results)

print("\nチャンク処理の実行:")
chunk_results = process_in_chunks(df_optimized)
print("チャンク処理完了")
print_memory_usage()

# メモリのクリーンアップ
del df_non_optimized, df_optimized, chunk_results
gc.collect()

print("\nクリーンアップ後のメモリ使用量:")
print_memory_usage()
```

### 3.2 計算最適化

**計算最適化とは？**
アルゴリズムやデータ構造を最適化して計算時間を短縮する手法です。

**Python実装例：**
```python
import numpy as np
import pandas as pd
from numba import jit
import time
from multiprocessing import Pool
import concurrent.futures

# 計算時間の測定
def timing_decorator(func):
    """関数の実行時間を測定するデコレーター"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"{func.__name__}の実行時間: {end_time - start_time:.4f}秒")
        return result
    return wrapper

# 非最適化関数
@timing_decorator
def slow_calculation(data):
    """非最適化された計算"""
    result = []
    for i in range(len(data)):
        sum_val = 0
        for j in range(len(data[i])):
            sum_val += data[i][j] ** 2
        result.append(np.sqrt(sum_val))
    return np.array(result)

# Numbaによる最適化
@jit(nopython=True)
def fast_calculation_numba(data):
    """Numbaによる最適化された計算"""
    result = np.zeros(len(data))
    for i in range(len(data)):
        sum_val = 0.0
        for j in range(len(data[i])):
            sum_val += data[i][j] ** 2
        result[i] = np.sqrt(sum_val)
    return result

@timing_decorator
def fast_calculation(data):
    """最適化された計算のラッパー"""
    return fast_calculation_numba(data)

# サンプルデータの作成
np.random.seed(42)
data = np.random.randn(10000, 100)

print("計算最適化の比較:")
print(f"データ形状: {data.shape}")

# 非最適化計算
result_slow = slow_calculation(data)

# 最適化計算
result_fast = fast_calculation(data)

# 結果の検証
print(f"結果の一致: {np.allclose(result_slow, result_fast)}")

# 並列処理による最適化
def parallel_calculation(data, n_processes=4):
    """並列処理による計算"""
    chunk_size = len(data) // n_processes
    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
    
    with Pool(n_processes) as pool:
        results = pool.map(fast_calculation_numba, chunks)
    
    return np.concatenate(results)

@timing_decorator
def parallel_calculation_wrapper(data):
    """並列計算のラッパー"""
    return parallel_calculation(data)

print("\n並列処理の実行:")
result_parallel = parallel_calculation_wrapper(data)

# 結果の検証
print(f"並列処理結果の一致: {np.allclose(result_fast, result_parallel)}")

# ベクトル化による最適化
@timing_decorator
def vectorized_calculation(data):
    """ベクトル化による最適化"""
    return np.sqrt(np.sum(data ** 2, axis=1))

print("\nベクトル化計算の実行:")
result_vectorized = vectorized_calculation(data)

# 結果の検証
print(f"ベクトル化結果の一致: {np.allclose(result_fast, result_vectorized)}")

# パフォーマンス比較
print("\nパフォーマンス比較:")
methods = ['非最適化', 'Numba', '並列処理', 'ベクトル化']
times = [0.1, 0.01, 0.02, 0.005]  # 仮の時間

plt.figure(figsize=(10, 6))
plt.bar(methods, times)
plt.ylabel('実行時間（秒）')
plt.title('計算最適化手法の比較')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

### 3.3 I/O最適化

**I/O最適化とは？**
データの読み書きを効率化する手法です。

**Python実装例：**
```python
import pandas as pd
import numpy as np
import h5py
import pickle
import json
import time
from pathlib import Path

# サンプルデータの作成
np.random.seed(42)
n_samples = 100000

data = {
    'user_id': np.random.randint(1, 1000000, n_samples),
    'age': np.random.normal(35, 10, n_samples),
    'income': np.random.normal(500, 150, n_samples),
    'category': np.random.choice(['A', 'B', 'C', 'D'], n_samples),
    'score': np.random.uniform(0, 100, n_samples)
}

df = pd.DataFrame(data)

print(f"データ形状: {df.shape}")
print(f"データサイズ: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

# 各種フォーマットでの保存と読み込み時間の比較
formats = {
    'CSV': {'save': lambda df, path: df.to_csv(path, index=False),
             'load': lambda path: pd.read_csv(path)},
    'Parquet': {'save': lambda df, path: df.to_parquet(path),
                'load': lambda path: pd.read_parquet(path)},
    'HDF5': {'save': lambda df, path: df.to_hdf(path, key='data'),
             'load': lambda path: pd.read_hdf(path, key='data')},
    'Pickle': {'save': lambda df, path: df.to_pickle(path),
               'load': lambda path: pd.read_pickle(path)},
    'JSON': {'save': lambda df, path: df.to_json(path),
             'load': lambda path: pd.read_json(path)}
}

# I/O性能の測定
io_results = {}

for format_name, format_funcs in formats.items():
    print(f"\n{format_name}フォーマットのテスト:")
    
    # 保存時間の測定
    save_path = f'test_data.{format_name.lower()}'
    start_time = time.time()
    format_funcs['save'](df, save_path)
    save_time = time.time() - start_time
    
    # 読み込み時間の測定
    start_time = time.time()
    loaded_df = format_funcs['load'](save_path)
    load_time = time.time() - start_time
    
    # ファイルサイズの測定
    file_size = Path(save_path).stat().st_size / 1024 / 1024  # MB
    
    io_results[format_name] = {
        'save_time': save_time,
        'load_time': load_time,
        'file_size': file_size
    }
    
    print(f"保存時間: {save_time:.4f}秒")
    print(f"読み込み時間: {load_time:.4f}秒")
    print(f"ファイルサイズ: {file_size:.2f} MB")
    
    # ファイルの削除
    Path(save_path).unlink()

# 結果の可視化
plt.figure(figsize=(15, 5))

# 保存時間の比較
plt.subplot(1, 3, 1)
formats_list = list(io_results.keys())
save_times = [io_results[f]['save_time'] for f in formats_list]
plt.bar(formats_list, save_times)
plt.ylabel('保存時間（秒）')
plt.title('保存時間の比較')
plt.xticks(rotation=45)

# 読み込み時間の比較
plt.subplot(1, 3, 2)
load_times = [io_results[f]['load_time'] for f in formats_list]
plt.bar(formats_list, load_times)
plt.ylabel('読み込み時間（秒）')
plt.title('読み込み時間の比較')
plt.xticks(rotation=45)

# ファイルサイズの比較
plt.subplot(1, 3, 3)
file_sizes = [io_results[f]['file_size'] for f in formats_list]
plt.bar(formats_list, file_sizes)
plt.ylabel('ファイルサイズ（MB）')
plt.title('ファイルサイズの比較')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# 最適なフォーマットの選択
best_format = min(io_results.items(), key=lambda x: x[1]['load_time'] + x[1]['save_time'])
print(f"\n最適なフォーマット: {best_format[0]}")

# チャンク読み込みによる最適化
def chunked_processing(file_path, chunk_size=10000):
    """チャンク読み込みによる処理"""
    results = []
    
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # チャンクでの処理
        chunk_result = chunk.groupby('category').agg({
            'age': 'mean',
            'income': 'mean',
            'score': 'mean'
        })
        results.append(chunk_result)
    
    return pd.concat(results)

# メモリ効率的なデータ処理
def memory_efficient_processing(file_path):
    """メモリ効率的なデータ処理"""
    total_age = 0
    total_income = 0
    total_score = 0
    count = 0
    
    for chunk in pd.read_csv(file_path, chunksize=10000):
        total_age += chunk['age'].sum()
        total_income += chunk['income'].sum()
        total_score += chunk['score'].sum()
        count += len(chunk)
    
    return {
        'avg_age': total_age / count,
        'avg_income': total_income / count,
        'avg_score': total_score / count
    }

print("\nメモリ効率的な処理の例:")
print("チャンク読み込みによる処理が可能")
print("大規模ファイルでもメモリ不足を回避")
```

## 4. まとめ

### 4.1 実装スキルの重要性

**各技術の役割：**

1. **分散処理**
   - 大規模データの効率的な処理
   - スケーラビリティの確保
   - 耐障害性の向上

2. **クラウドML**
   - インフラ管理の簡素化
   - 自動スケーリング
   - コスト最適化

3. **パフォーマンス最適化**
   - 処理速度の向上
   - リソース使用量の削減
   - コスト効率の改善

### 4.2 実践的なポイント

1. **適切な技術選択**: 問題に応じた最適な技術の選択
2. **段階的な最適化**: 計測→改善→検証のサイクル
3. **コスト効率**: 性能とコストのバランス
4. **保守性**: 長期的な運用を考慮した設計

### 4.3 次のステップ

1. **基本的な分散処理**をマスター
2. **クラウドプラットフォーム**の活用
3. **パフォーマンス最適化**の実践
4. **実践的なプロジェクト**での応用

実装スキルは、機械学習システムを実際に運用する上で非常に重要です。理論と実践の両方を理解することで、効率的でスケーラブルなシステムを構築できます。 