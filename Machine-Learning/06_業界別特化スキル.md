# 業界別特化スキル - 実務で使える機械学習技術

## 1. レコメンデーションシステム

### 1.1 レコメンデーションシステムとは？

**レコメンデーションシステムとは？**
ユーザーに適切な商品やコンテンツを推薦するシステムです。

**応用例：**
- Amazonの商品推薦
- Netflixの映画推薦
- Spotifyの音楽推薦
- YouTubeの動画推薦

**なぜ重要なのでしょうか？**

1. **ユーザー体験の向上**: ユーザーが欲しいものを簡単に見つけられる
2. **売上の向上**: 関連商品の購買促進
3. **顧客満足度**: パーソナライズされた体験
4. **ビジネス価値**: 直接的な収益向上

### 1.2 協調フィルタリング

**協調フィルタリングとは？**
ユーザーの行動パターンから推薦を行う手法です。

#### 1.2.1 User-based協調フィルタリング

**User-based協調フィルタリングとは？**
似た行動パターンのユーザーが好むアイテムを推薦する手法です。

**Python実装例：**
```python
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# サンプルデータ（ユーザー×アイテムの評価行列）
np.random.seed(42)
n_users = 100
n_items = 50

# 評価行列の作成（1-5の評価、欠損値あり）
ratings_matrix = np.random.choice([1, 2, 3, 4, 5, np.nan], 
                                 size=(n_users, n_items), 
                                 p=[0.1, 0.1, 0.2, 0.3, 0.2, 0.1])

# DataFrameに変換
ratings_df = pd.DataFrame(ratings_matrix, 
                         columns=[f'item_{i}' for i in range(n_items)],
                         index=[f'user_{i}' for i in range(n_users)])

print("評価行列の例（最初の10×10）:")
print(ratings_df.iloc[:10, :10])

# ユーザー間の類似度計算
def calculate_user_similarity(ratings_df):
    """ユーザー間の類似度を計算"""
    # 欠損値を0で埋める
    ratings_filled = ratings_df.fillna(0)
    
    # コサイン類似度の計算
    user_similarity = cosine_similarity(ratings_filled)
    
    return user_similarity

user_similarity = calculate_user_similarity(ratings_df)

print(f"\nユーザー類似度行列の形状: {user_similarity.shape}")
print(f"類似度の範囲: {user_similarity.min():.3f} - {user_similarity.max():.3f}")

# 類似度の分布
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.hist(user_similarity.flatten(), bins=50, alpha=0.7)
plt.xlabel('類似度')
plt.ylabel('頻度')
plt.title('ユーザー類似度の分布')

plt.subplot(1, 2, 2)
plt.imshow(user_similarity[:20, :20], cmap='viridis')
plt.colorbar(label='類似度')
plt.title('ユーザー類似度行列（20×20）')
plt.tight_layout()
plt.show()

# User-based推薦の実装
def user_based_recommendation(ratings_df, user_similarity, user_id, item_id, k=5):
    """User-based推薦の実装"""
    user_idx = list(ratings_df.index).index(user_id)
    
    # 類似ユーザーの取得
    similar_users = np.argsort(user_similarity[user_idx])[::-1][1:k+1]
    
    # 類似ユーザーの評価を取得
    similar_ratings = []
    similar_weights = []
    
    for similar_user_idx in similar_users:
        rating = ratings_df.iloc[similar_user_idx][item_id]
        if not pd.isna(rating):
            similar_ratings.append(rating)
            similar_weights.append(user_similarity[user_idx, similar_user_idx])
    
    if len(similar_ratings) == 0:
        return np.nan
    
    # 重み付き平均で予測
    predicted_rating = np.average(similar_ratings, weights=similar_weights)
    
    return predicted_rating

# 推薦の例
user_id = 'user_0'
item_id = 'item_0'
predicted_rating = user_based_recommendation(ratings_df, user_similarity, user_id, item_id)

print(f"\n推薦例:")
print(f"ユーザー: {user_id}")
print(f"アイテム: {item_id}")
print(f"予測評価: {predicted_rating:.2f}")
```

#### 1.2.2 Item-based協調フィルタリング

**Item-based協調フィルタリングとは？**
アイテム間の類似度を計算して推薦を行う手法です。

**Python実装例：**
```python
def calculate_item_similarity(ratings_df):
    """アイテム間の類似度を計算"""
    # 欠損値を0で埋める
    ratings_filled = ratings_df.fillna(0)
    
    # アイテム間のコサイン類似度を計算
    item_similarity = cosine_similarity(ratings_filled.T)
    
    return item_similarity

item_similarity = calculate_item_similarity(ratings_df)

print(f"アイテム類似度行列の形状: {item_similarity.shape}")

# Item-based推薦の実装
def item_based_recommendation(ratings_df, item_similarity, user_id, item_id, k=5):
    """Item-based推薦の実装"""
    item_idx = list(ratings_df.columns).index(item_id)
    
    # 類似アイテムの取得
    similar_items = np.argsort(item_similarity[item_idx])[::-1][1:k+1]
    
    # 類似アイテムの評価を取得
    similar_ratings = []
    similar_weights = []
    
    for similar_item_idx in similar_items:
        rating = ratings_df.loc[user_id].iloc[similar_item_idx]
        if not pd.isna(rating):
            similar_ratings.append(rating)
            similar_weights.append(item_similarity[item_idx, similar_item_idx])
    
    if len(similar_ratings) == 0:
        return np.nan
    
    # 重み付き平均で予測
    predicted_rating = np.average(similar_ratings, weights=similar_weights)
    
    return predicted_rating

# 推薦の例
predicted_rating_item = item_based_recommendation(ratings_df, item_similarity, user_id, item_id)

print(f"\nItem-based推薦:")
print(f"ユーザー: {user_id}")
print(f"アイテム: {item_id}")
print(f"予測評価: {predicted_rating_item:.2f}")
```

### 1.3 コンテンツベースフィルタリング

**コンテンツベースフィルタリングとは？**
アイテムの特徴量から類似性を計算して推薦を行う手法です。

**Python実装例：**
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# アイテムの特徴量（例：映画のジャンル、監督、俳優など）
movie_features = {
    'movie_1': {'genre': 'action', 'director': 'spielberg', 'year': 1990},
    'movie_2': {'genre': 'action', 'director': 'cameron', 'year': 1991},
    'movie_3': {'genre': 'drama', 'director': 'nolan', 'year': 2000},
    'movie_4': {'genre': 'action', 'director': 'spielberg', 'year': 1993},
    'movie_5': {'genre': 'comedy', 'director': 'allen', 'year': 1985}
}

# 特徴量をテキストに変換
movie_descriptions = []
movie_ids = []

for movie_id, features in movie_features.items():
    description = f"{features['genre']} {features['director']} {features['year']}"
    movie_descriptions.append(description)
    movie_ids.append(movie_id)

# TF-IDFベクトライザー
vectorizer = TfidfVectorizer()
movie_vectors = vectorizer.fit_transform(movie_descriptions)

# アイテム間の類似度計算
movie_similarity = cosine_similarity(movie_vectors)

# コンテンツベース推薦
def content_based_recommendation(movie_similarity, movie_ids, target_movie_id, k=3):
    """コンテンツベース推薦の実装"""
    target_idx = movie_ids.index(target_movie_id)
    
    # 類似アイテムの取得
    similar_movies = np.argsort(movie_similarity[target_idx])[::-1][1:k+1]
    
    recommendations = []
    for idx in similar_movies:
        recommendations.append({
            'movie_id': movie_ids[idx],
            'similarity': movie_similarity[target_idx, idx]
        })
    
    return recommendations

# 推薦の例
target_movie = 'movie_1'
recommendations = content_based_recommendation(movie_similarity, movie_ids, target_movie)

print(f"\nコンテンツベース推薦:")
print(f"ターゲット映画: {target_movie}")
print("推薦映画:")
for rec in recommendations:
    print(f"- {rec['movie_id']}: 類似度 {rec['similarity']:.3f}")
```

### 1.4 ハイブリッド推薦システム

**ハイブリッド推薦システムとは？**
複数の推薦手法を組み合わせて性能を向上させる手法です。

**Python実装例：**
```python
def hybrid_recommendation(ratings_df, user_similarity, item_similarity, 
                         movie_similarity, movie_ids, user_id, item_id, 
                         alpha=0.5, k=5):
    """ハイブリッド推薦の実装"""
    
    # User-based推薦
    user_pred = user_based_recommendation(ratings_df, user_similarity, user_id, item_id, k)
    
    # Item-based推薦
    item_pred = item_based_recommendation(ratings_df, item_similarity, user_id, item_id, k)
    
    # コンテンツベース推薦（アイテムが映画の場合）
    if item_id in movie_ids:
        content_recs = content_based_recommendation(movie_similarity, movie_ids, item_id, k)
        content_score = np.mean([rec['similarity'] for rec in content_recs])
    else:
        content_score = 0.5  # デフォルト値
    
    # 重み付き平均
    if not pd.isna(user_pred) and not pd.isna(item_pred):
        hybrid_pred = alpha * user_pred + (1 - alpha) * item_pred
    elif not pd.isna(user_pred):
        hybrid_pred = user_pred
    elif not pd.isna(item_pred):
        hybrid_pred = item_pred
    else:
        hybrid_pred = content_score * 5  # スケール調整
    
    return hybrid_pred

# ハイブリッド推薦の例
hybrid_pred = hybrid_recommendation(ratings_df, user_similarity, item_similarity, 
                                   movie_similarity, movie_ids, user_id, item_id)

print(f"\nハイブリッド推薦:")
print(f"ユーザー: {user_id}")
print(f"アイテム: {item_id}")
print(f"予測評価: {hybrid_pred:.2f}")

# 手法比較
print(f"\n手法比較:")
print(f"User-based: {predicted_rating:.2f}")
print(f"Item-based: {predicted_rating_item:.2f}")
print(f"Hybrid: {hybrid_pred:.2f}")
```

## 2. 不正検知システム

### 2.1 不正検知とは？

**不正検知とは？**
異常なパターンや行動を自動的に検出するシステムです。

**応用例：**
- クレジットカード不正利用検知
- ネットワーク侵入検知
- 保険金詐欺検知
- 内部不正検知

**なぜ重要なのでしょうか？**

1. **リスク軽減**: 被害の早期発見と防止
2. **コスト削減**: 人的監視の自動化
3. **リアルタイム対応**: 即座の異常検知
4. **スケーラビリティ**: 大量データの処理

### 2.2 異常検知アルゴリズム

#### 2.2.1 Isolation Forest

**Isolation Forestとは？**
異常値を効率的に検出する手法です。

**Python実装例：**
```python
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt

# 不正検知用のサンプルデータ
np.random.seed(42)
n_samples = 1000

# 正常な取引データ
normal_transactions = np.random.normal([100, 50, 30], [20, 10, 5], (n_samples, 3))

# 異常な取引データ（不正）
anomaly_transactions = np.random.normal([500, 200, 100], [50, 20, 10], (50, 3))

# データの結合
all_transactions = np.vstack([normal_transactions, anomaly_transactions])
labels = np.concatenate([np.zeros(n_samples), np.ones(50)])  # 0: 正常, 1: 異常

print(f"データ形状: {all_transactions.shape}")
print(f"正常取引: {np.sum(labels == 0)}")
print(f"異常取引: {np.sum(labels == 1)}")

# データの標準化
scaler = StandardScaler()
transactions_scaled = scaler.fit_transform(all_transactions)

# Isolation Forestによる異常検知
iso_forest = IsolationForest(
    contamination=0.05,  # 異常の割合
    random_state=42
)

anomaly_scores = iso_forest.fit_predict(transactions_scaled)

# 結果の評価
from sklearn.metrics import classification_report, confusion_matrix

# Isolation Forestの結果を正解ラベルに変換（-1: 異常, 1: 正常）
predicted_labels = (anomaly_scores == 1).astype(int)

print("Isolation Forestの結果:")
print(classification_report(labels, predicted_labels))

# 結果の可視化
plt.figure(figsize=(15, 5))

# 元データの分布
plt.subplot(1, 3, 1)
plt.scatter(all_transactions[:, 0], all_transactions[:, 1], 
           c=labels, alpha=0.6, cmap='viridis')
plt.xlabel('取引金額')
plt.ylabel('取引回数')
plt.title('元データ（色: 異常度）')
plt.colorbar(label='異常度')

# 異常検知結果
plt.subplot(1, 3, 2)
colors = ['blue' if score == 1 else 'red' for score in anomaly_scores]
plt.scatter(all_transactions[:, 0], all_transactions[:, 1], 
           c=colors, alpha=0.6)
plt.xlabel('取引金額')
plt.ylabel('取引回数')
plt.title('異常検知結果（赤: 異常、青: 正常）')

# 異常スコアの分布
plt.subplot(1, 3, 3)
plt.hist(anomaly_scores, bins=20, alpha=0.7)
plt.xlabel('異常スコア')
plt.ylabel('頻度')
plt.title('異常スコアの分布')

plt.tight_layout()
plt.show()
```

#### 2.2.2 One-Class SVM

**One-Class SVMとは？**
正常データのみで学習し、異常を検出する手法です。

**Python実装例：**
```python
from sklearn.svm import OneClassSVM

# 正常データのみで学習
normal_data = all_transactions[labels == 0]

# One-Class SVM
one_class_svm = OneClassSVM(
    nu=0.05,  # 異常の割合
    kernel='rbf',
    gamma='scale'
)

# 正常データで学習
one_class_svm.fit(normal_data)

# 全データで予測
svm_scores = one_class_svm.predict(all_transactions)

# 結果の評価
svm_predicted_labels = (svm_scores == 1).astype(int)

print("One-Class SVMの結果:")
print(classification_report(labels, svm_predicted_labels))

# 手法比較
print("手法比較:")
print("Isolation Forest:")
print(classification_report(labels, predicted_labels, output_dict=True)['weighted avg']['f1-score'])
print("One-Class SVM:")
print(classification_report(labels, svm_predicted_labels, output_dict=True)['weighted avg']['f1-score'])
```

#### 2.2.3 Autoencoder

**Autoencoderとは？**
ニューラルネットワークを用いた異常検知手法です。

**Python実装例：**
```python
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

# Autoencoderの構築
def create_autoencoder(input_dim):
    """Autoencoderモデルの作成"""
    # エンコーダー
    input_layer = Input(shape=(input_dim,))
    encoded = Dense(64, activation='relu')(input_layer)
    encoded = Dense(32, activation='relu')(encoded)
    encoded = Dense(16, activation='relu')(encoded)
    
    # デコーダー
    decoded = Dense(32, activation='relu')(encoded)
    decoded = Dense(64, activation='relu')(decoded)
    decoded = Dense(input_dim, activation='sigmoid')(decoded)
    
    # モデルの構築
    autoencoder = Model(input_layer, decoded)
    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    
    return autoencoder

# データの準備
normal_data_scaled = scaler.transform(normal_data)
all_data_scaled = scaler.transform(all_transactions)

# Autoencoderの学習
autoencoder = create_autoencoder(normal_data_scaled.shape[1])
autoencoder.fit(normal_data_scaled, normal_data_scaled, 
                epochs=50, batch_size=32, verbose=0)

# 再構築誤差の計算
reconstruction_error = autoencoder.evaluate(all_data_scaled, all_data_scaled, verbose=0)
print(f"再構築誤差: {reconstruction_error:.4f}")

# 異常スコアの計算（再構築誤差）
reconstructed = autoencoder.predict(all_data_scaled)
mse_scores = np.mean((all_data_scaled - reconstructed) ** 2, axis=1)

# 異常検知（閾値ベース）
threshold = np.percentile(mse_scores, 95)  # 上位5%を異常とする
autoencoder_predicted = (mse_scores > threshold).astype(int)

print("Autoencoderの結果:")
print(classification_report(labels, autoencoder_predicted))

# 結果の可視化
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.scatter(all_transactions[:, 0], all_transactions[:, 1], 
           c=mse_scores, alpha=0.6, cmap='viridis')
plt.colorbar(label='再構築誤差')
plt.xlabel('取引金額')
plt.ylabel('取引回数')
plt.title('Autoencoder - 再構築誤差')

plt.subplot(1, 3, 2)
plt.hist(mse_scores, bins=50, alpha=0.7)
plt.axvline(threshold, color='red', linestyle='--', label=f'閾値: {threshold:.4f}')
plt.xlabel('再構築誤差')
plt.ylabel('頻度')
plt.title('再構築誤差の分布')
plt.legend()

plt.subplot(1, 3, 3)
colors = ['blue' if pred == 0 else 'red' for pred in autoencoder_predicted]
plt.scatter(all_transactions[:, 0], all_transactions[:, 1], 
           c=colors, alpha=0.6)
plt.xlabel('取引金額')
plt.ylabel('取引回数')
plt.title('Autoencoder - 異常検知結果')

plt.tight_layout()
plt.show()
```

### 2.3 時系列異常検知

**時系列異常検知とは？**
時系列データにおける異常パターンを検出する手法です。

**Python実装例：**
```python
from statsmodels.tsa.seasonal import seasonal_decompose
from scipy import stats

# 時系列データの作成
np.random.seed(42)
n_points = 1000

# 正常な時系列データ
trend = np.linspace(100, 120, n_points)
seasonal = 10 * np.sin(2 * np.pi * np.arange(n_points) / 100)
noise = np.random.normal(0, 2, n_points)
normal_series = trend + seasonal + noise

# 異常な時系列データ
anomaly_indices = [200, 400, 600, 800]
for idx in anomaly_indices:
    normal_series[idx] += np.random.normal(30, 5)  # 異常値の追加

# 時系列異常検知
def detect_timeseries_anomalies(series, window_size=50):
    """時系列異常検知の実装"""
    anomalies = []
    
    for i in range(window_size, len(series)):
        # 移動平均と標準偏差の計算
        window = series[i-window_size:i]
        mean_val = np.mean(window)
        std_val = np.std(window)
        
        # 現在の値が異常かどうかを判定
        current_val = series[i]
        z_score = abs(current_val - mean_val) / std_val
        
        if z_score > 3:  # 3シグマルール
            anomalies.append(i)
    
    return anomalies

# 異常検知の実行
detected_anomalies = detect_timeseries_anomalies(normal_series)

print(f"検出された異常点: {detected_anomalies}")
print(f"実際の異常点: {anomaly_indices}")

# 結果の可視化
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.plot(normal_series, label='時系列データ')
plt.scatter(anomaly_indices, normal_series[anomaly_indices], 
           color='red', s=100, label='実際の異常')
plt.xlabel('時間')
plt.ylabel('値')
plt.title('時系列データと実際の異常')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(normal_series, label='時系列データ')
plt.scatter(detected_anomalies, normal_series[detected_anomalies], 
           color='orange', s=100, label='検出された異常')
plt.xlabel('時間')
plt.ylabel('値')
plt.title('時系列データと検出された異常')
plt.legend()

plt.tight_layout()
plt.show()
```

## 3. 予知保全システム

### 3.1 予知保全とは？

**予知保全とは？**
設備の故障を事前に予測し、適切なタイミングでメンテナンスを行う手法です。

**応用例：**
- 製造設備の故障予測
- 航空機エンジンの状態監視
- 発電所の設備監視
- 自動車の部品劣化予測

**なぜ重要なのでしょうか？**

1. **コスト削減**: 計画外停止の防止
2. **安全性向上**: 重大事故の防止
3. **効率化**: メンテナンススケジュールの最適化
4. **リソース最適化**: 部品在庫の適正管理

### 3.2 センサーデータ分析

**センサーデータ分析とは？**
設備から取得されるセンサーデータを分析して異常を検出する手法です。

**Python実装例：**
```python
import pandas as pd
from datetime import datetime, timedelta

# センサーデータの生成
np.random.seed(42)
n_samples = 1000

# 時間インデックス
timestamps = pd.date_range('2024-01-01', periods=n_samples, freq='H')

# 正常なセンサーデータ
temperature = np.random.normal(50, 5, n_samples)
vibration = np.random.normal(0.1, 0.02, n_samples)
pressure = np.random.normal(100, 10, n_samples)
current = np.random.normal(20, 2, n_samples)

# 故障の兆候を追加
fault_start = 800
for i in range(fault_start, n_samples):
    # 温度の上昇
    temperature[i] += (i - fault_start) * 0.1
    # 振動の増加
    vibration[i] += (i - fault_start) * 0.001
    # 圧力の変動
    pressure[i] += np.random.normal(0, 2)
    # 電流の変動
    current[i] += np.random.normal(0, 0.5)

# センサーデータのDataFrame作成
sensor_data = pd.DataFrame({
    'timestamp': timestamps,
    'temperature': temperature,
    'vibration': vibration,
    'pressure': pressure,
    'current': current
})

print("センサーデータの例:")
print(sensor_data.head())

# センサーデータの可視化
plt.figure(figsize=(15, 10))

sensors = ['temperature', 'vibration', 'pressure', 'current']
for i, sensor in enumerate(sensors):
    plt.subplot(2, 2, i+1)
    plt.plot(sensor_data['timestamp'], sensor_data[sensor])
    plt.axvline(sensor_data['timestamp'][fault_start], color='red', 
                linestyle='--', label='故障開始')
    plt.xlabel('時間')
    plt.ylabel(sensor)
    plt.title(f'{sensor}の時系列')
    plt.legend()

plt.tight_layout()
plt.show()

# 特徴量エンジニアリング
def create_features(df):
    """センサーデータから特徴量を作成"""
    features = df.copy()
    
    # 移動平均
    for col in ['temperature', 'vibration', 'pressure', 'current']:
        features[f'{col}_ma_5'] = features[col].rolling(window=5).mean()
        features[f'{col}_ma_10'] = features[col].rolling(window=10).mean()
    
    # 移動標準偏差
    for col in ['temperature', 'vibration', 'pressure', 'current']:
        features[f'{col}_std_5'] = features[col].rolling(window=5).std()
        features[f'{col}_std_10'] = features[col].rolling(window=10).std()
    
    # 変化率
    for col in ['temperature', 'vibration', 'pressure', 'current']:
        features[f'{col}_change'] = features[col].diff()
        features[f'{col}_change_rate'] = features[col].pct_change()
    
    # ラグ特徴量
    for col in ['temperature', 'vibration', 'pressure', 'current']:
        features[f'{col}_lag_1'] = features[col].shift(1)
        features[f'{col}_lag_2'] = features[col].shift(2)
    
    return features

# 特徴量の作成
sensor_features = create_features(sensor_data)

print("特徴量エンジニアリング後のデータ形状:")
print(sensor_features.shape)
print(f"特徴量数: {sensor_features.shape[1] - 1}")  # timestampを除く
```

### 3.3 故障予測モデル

**故障予測モデルとは？**
センサーデータから故障の発生を予測する機械学習モデルです。

**Python実装例：**
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# 故障ラベルの作成
fault_labels = np.zeros(len(sensor_features))
fault_labels[fault_start:] = 1  # 故障開始点以降を故障とする

# 特徴量とラベルの準備
feature_cols = [col for col in sensor_features.columns if col != 'timestamp']
X = sensor_features[feature_cols].fillna(0)
y = fault_labels

# データ分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 故障予測モデルの学習
fault_model = RandomForestClassifier(n_estimators=100, random_state=42)
fault_model.fit(X_train, y_train)

# 予測
y_pred = fault_model.predict(X_test)

# 評価
print("故障予測モデルの評価:")
print(classification_report(y_test, y_pred))

# 特徴量の重要度
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': fault_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n特徴量の重要度（上位10件）:")
print(feature_importance.head(10))

# 結果の可視化
plt.figure(figsize=(15, 5))

# 特徴量重要度
plt.subplot(1, 3, 1)
top_features = feature_importance.head(10)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('重要度')
plt.title('特徴量の重要度（上位10件）')

# 混同行列
plt.subplot(1, 3, 2)
cm = confusion_matrix(y_test, y_pred)
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('混同行列')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['正常', '故障'])
plt.yticks(tick_marks, ['正常', '故障'])
plt.ylabel('実際の値')
plt.xlabel('予測値')

# 時系列での予測結果
plt.subplot(1, 3, 3)
test_indices = np.arange(len(y_test))
plt.plot(test_indices, y_test, label='実際', alpha=0.7)
plt.plot(test_indices, y_pred, label='予測', alpha=0.7)
plt.xlabel('サンプル')
plt.ylabel('故障状態')
plt.title('時系列での予測結果')
plt.legend()

plt.tight_layout()
plt.show()
```

### 3.4 メンテナンススケジュール最適化

**メンテナンススケジュール最適化とは？**
故障予測結果を基に、最適なメンテナンスタイミングを決定する手法です。

**Python実装例：**
```python
def optimize_maintenance_schedule(predictions, confidence_scores, 
                                maintenance_cost=1000, failure_cost=10000):
    """メンテナンススケジュールの最適化"""
    
    schedule = []
    total_cost = 0
    
    for i, (pred, conf) in enumerate(zip(predictions, confidence_scores)):
        if pred == 1:  # 故障予測
            if conf > 0.8:  # 高信頼度
                schedule.append({
                    'time': i,
                    'action': 'immediate_maintenance',
                    'cost': maintenance_cost,
                    'confidence': conf
                })
                total_cost += maintenance_cost
            elif conf > 0.6:  # 中信頼度
                schedule.append({
                    'time': i,
                    'action': 'scheduled_maintenance',
                    'cost': maintenance_cost * 0.8,
                    'confidence': conf
                })
                total_cost += maintenance_cost * 0.8
            else:  # 低信頼度
                schedule.append({
                    'time': i,
                    'action': 'monitor',
                    'cost': 0,
                    'confidence': conf
                })
    
    return schedule, total_cost

# 予測確率の取得
prediction_proba = fault_model.predict_proba(X_test)
confidence_scores = np.max(prediction_proba, axis=1)

# メンテナンススケジュールの最適化
schedule, total_cost = optimize_maintenance_schedule(
    y_pred, confidence_scores
)

print("メンテナンススケジュール:")
for item in schedule[:10]:  # 最初の10件を表示
    print(f"時間: {item['time']}, アクション: {item['action']}, "
          f"コスト: {item['cost']}, 信頼度: {item['confidence']:.3f}")

print(f"\n総コスト: {total_cost}")

# コスト分析
maintenance_actions = [item['action'] for item in schedule]
action_counts = pd.Series(maintenance_actions).value_counts()

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
action_counts.plot(kind='bar')
plt.title('メンテナンスアクションの分布')
plt.xlabel('アクション')
plt.ylabel('回数')
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
costs = [item['cost'] for item in schedule]
plt.hist(costs, bins=20, alpha=0.7)
plt.xlabel('コスト')
plt.ylabel('頻度')
plt.title('メンテナンスコストの分布')

plt.tight_layout()
plt.show()
```

## 4. まとめ

### 4.1 業界別スキルの重要性

**各業界での応用：**

1. **レコメンデーションシステム**
   - EC業界：商品推薦
   - メディア業界：コンテンツ推薦
   - 金融業界：金融商品推薦

2. **不正検知システム**
   - 金融業界：クレジットカード不正利用
   - 保険業界：保険金詐欺
   - 製造業界：内部不正

3. **予知保全システム**
   - 製造業界：設備故障予測
   - エネルギー業界：発電設備監視
   - 運輸業界：車両メンテナンス

### 4.2 実践的なポイント

1. **ドメイン知識**: 業界特有の知識の活用
2. **データ品質**: 高品質なデータの確保
3. **リアルタイム性**: 即座の対応が必要
4. **スケーラビリティ**: 大規模システムへの対応

### 4.3 次のステップ

1. **基本的な手法**をマスター
2. **業界特化の知識**を習得
3. **実践的なプロジェクト**に取り組む
4. **継続的な学習**を実践

業界別特化スキルは、機械学習を実務で活用する上で非常に重要です。理論と実践の両方を理解することで、価値のあるシステムを構築できます。 