# 機械学習アルゴリズム基礎 - データから知識を抽出する技術

## 1. 機械学習とは？

### 1.1 機械学習の定義

機械学習とは、**「データから自動的にパターンを学習し、新しいデータに対して予測や判断を行う技術」**です。

**従来のプログラミングとの違い：**
```
従来のプログラミング：
入力 → 人間が作ったルール → 出力

機械学習：
入力 → データから学習したルール → 出力
```

**例：スパムメール判定**
```
従来の方法：
- 「無料」という単語が含まれていたらスパム
- 「当選」という単語が含まれていたらスパム
- 送信者が不明だったらスパム

機械学習：
- 大量のメールデータ（スパム/非スパム）を学習
- 自動的にパターンを発見
- 新しいメールに対して自動判定
```

### 1.2 機械学習の種類

機械学習は大きく3つに分類されます：

1. **教師あり学習**: 正解データがある場合
2. **教師なし学習**: 正解データがない場合
3. **強化学習**: 試行錯誤で学習する場合

## 2. 教師あり学習

### 2.1 分類問題

**分類問題とは？**
データを複数のカテゴリに分ける問題です。

**例：**
- メールのスパム判定（スパム/非スパム）
- 画像の犬猫判定（犬/猫）
- 顧客の購買予測（購買する/しない）

#### 2.1.1 Random Forest（ランダムフォレスト）

**Random Forestとは？**
複数の決定木を組み合わせたアンサンブル学習手法です。

**仕組み：**
1. 複数の決定木を作成（各木は異なるデータサンプルを使用）
2. 各木が独立して予測を行う
3. 多数決で最終予測を決定

**Python実装例：**
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np

# サンプルデータ（顧客の購買予測）
np.random.seed(42)
n_samples = 1000

# 特徴量
age = np.random.normal(35, 10, n_samples)
income = np.random.normal(500, 150, n_samples)
purchase_history = np.random.poisson(5, n_samples)

# 目的変数（購買するかどうか）
# 年齢が若く、収入が高く、購入履歴が多いほど購買確率が高い
purchase_prob = 1 / (1 + np.exp(-(0.1*age + 0.002*income + 0.3*purchase_history - 20)))
purchase = (np.random.random(n_samples) < purchase_prob).astype(int)

# データフレーム作成
df = pd.DataFrame({
    'age': age,
    'income': income,
    'purchase_history': purchase_history,
    'purchase': purchase
})

# データ分割
X = df[['age', 'income', 'purchase_history']]
y = df['purchase']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forestモデルの学習
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 予測
y_pred = rf_model.predict(X_test)

# 評価
accuracy = accuracy_score(y_test, y_pred)
print(f"正解率: {accuracy:.3f}")

# 特徴量の重要度
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n特徴量の重要度:")
print(feature_importance)
```

#### 2.1.2 XGBoost（eXtreme Gradient Boosting）

**XGBoostとは？**
勾配ブースティングの高性能な実装です。Kaggleコンペティションでよく使われます。

**特徴：**
- 高い予測精度
- 過学習の防止機能
- 欠損値の自動処理
- 並列計算による高速化

**Python実装例：**
```python
import xgboost as xgb
from sklearn.metrics import classification_report

# XGBoostモデルの学習
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42
)
xgb_model.fit(X_train, y_train)

# 予測
y_pred_xgb = xgb_model.predict(X_test)

# 評価
print("XGBoostの結果:")
print(classification_report(y_test, y_pred_xgb))

# 特徴量の重要度
xgb_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nXGBoost特徴量の重要度:")
print(xgb_importance)
```

#### 2.1.3 SVM（Support Vector Machine）

**SVMとは？**
データを高次元空間に変換して、境界線で分類する手法です。

**仕組み：**
1. データを高次元空間に変換（カーネル関数使用）
2. マージン（境界線とデータの距離）を最大化
3. 最適な境界線を見つける

**Python実装例：**
```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# データの標準化（SVMはスケールに敏感）
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SVMモデルの学習
svm_model = SVC(kernel='rbf', C=1.0, random_state=42)
svm_model.fit(X_train_scaled, y_train)

# 予測
y_pred_svm = svm_model.predict(X_test_scaled)

# 評価
print("SVMの結果:")
print(classification_report(y_test, y_pred_svm))
```

#### 2.1.4 Neural Networks（ニューラルネットワーク）

**Neural Networksとは？**
人間の脳の神経細胞を模倣した学習手法です。

**仕組み：**
1. 入力層：データを受け取る
2. 隠れ層：パターンを学習
3. 出力層：予測結果を出力

**Python実装例：**
```python
from sklearn.neural_network import MLPClassifier

# ニューラルネットワークモデルの学習
nn_model = MLPClassifier(
    hidden_layer_sizes=(100, 50),  # 隠れ層の構造
    activation='relu',              # 活性化関数
    solver='adam',                  # 最適化アルゴリズム
    max_iter=500,                   # 最大反復回数
    random_state=42
)
nn_model.fit(X_train_scaled, y_train)

# 予測
y_pred_nn = nn_model.predict(X_test_scaled)

# 評価
print("ニューラルネットワークの結果:")
print(classification_report(y_test, y_pred_nn))
```

### 2.2 回帰問題

**回帰問題とは？**
連続値を予測する問題です。

**例：**
- 住宅価格の予測
- 売上の予測
- 気温の予測

#### 2.2.1 Linear Regression（線形回帰）

**Linear Regressionとは？**
最も基本的な回帰手法です。直線で予測を行います。

**数式：**
```
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
```

**Python実装例：**
```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 住宅価格予測のサンプルデータ
np.random.seed(42)
n_samples = 1000

# 特徴量
size = np.random.normal(100, 30, n_samples)  # 面積（㎡）
age = np.random.normal(20, 10, n_samples)    # 築年数
distance = np.random.normal(500, 200, n_samples)  # 駅からの距離（m）

# 目的変数（住宅価格）
price = 3000 + 50*size - 20*age - 0.5*distance + np.random.normal(0, 100, n_samples)

# データフレーム作成
df_house = pd.DataFrame({
    'size': size,
    'age': age,
    'distance': distance,
    'price': price
})

# データ分割
X_house = df_house[['size', 'age', 'distance']]
y_house = df_house['price']
X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(
    X_house, y_house, test_size=0.2, random_state=42
)

# 線形回帰モデルの学習
lr_model = LinearRegression()
lr_model.fit(X_train_house, y_train_house)

# 予測
y_pred_lr = lr_model.predict(X_test_house)

# 評価
mse = mean_squared_error(y_test_house, y_pred_lr)
r2 = r2_score(y_test_house, y_pred_lr)

print(f"平均二乗誤差: {mse:.2f}")
print(f"決定係数: {r2:.3f}")

# 係数
coefficients = pd.DataFrame({
    'feature': X_house.columns,
    'coefficient': lr_model.coef_
})
print("\n係数:")
print(coefficients)
```

#### 2.2.2 Ridge/Lasso回帰

**Ridge/Lassoとは？**
正則化を加えた線形回帰です。過学習を防ぎます。

**Ridge回帰（L2正則化）：**
```
損失関数 = MSE + α × Σ(βᵢ)²
```

**Lasso回帰（L1正則化）：**
```
損失関数 = MSE + α × Σ|βᵢ|
```

**Python実装例：**
```python
from sklearn.linear_model import Ridge, Lasso

# Ridge回帰
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train_house, y_train_house)
y_pred_ridge = ridge_model.predict(X_test_house)

# Lasso回帰
lasso_model = Lasso(alpha=1.0)
lasso_model.fit(X_train_house, y_train_house)
y_pred_lasso = lasso_model.predict(X_test_house)

# 比較
print("モデル比較:")
print(f"線形回帰 R²: {r2_score(y_test_house, y_pred_lr):.3f}")
print(f"Ridge回帰 R²: {r2_score(y_test_house, y_pred_ridge):.3f}")
print(f"Lasso回帰 R²: {r2_score(y_test_house, y_pred_lasso):.3f}")
```

#### 2.2.3 Gradient Boosting

**Gradient Boostingとは？**
弱い学習器を順次追加して、予測精度を向上させる手法です。

**Python実装例：**
```python
from sklearn.ensemble import GradientBoostingRegressor

# Gradient Boostingモデルの学習
gb_model = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42
)
gb_model.fit(X_train_house, y_train_house)

# 予測
y_pred_gb = gb_model.predict(X_test_house)

# 評価
print(f"Gradient Boosting R²: {r2_score(y_test_house, y_pred_gb):.3f}")
```

### 2.3 ランキング学習

**ランキング学習とは？**
アイテムの順序を予測する学習手法です。

**例：**
- 検索結果のランキング
- 商品推薦の順序
- 広告の表示順序

#### 2.3.1 Learning to Rank

**Learning to Rankとは？**
ランキング問題に特化した機械学習手法です。

**Python実装例：**
```python
import lightgbm as lgb

# ランキング学習用のサンプルデータ
np.random.seed(42)
n_queries = 100
n_docs_per_query = 10

# クエリとドキュメントの特徴量
queries = []
groups = []
features = []
labels = []

for i in range(n_queries):
    # 各クエリに対して10個のドキュメント
    query_features = np.random.randn(n_docs_per_query, 5)  # 5次元特徴量
    query_labels = np.random.randint(0, 4, n_docs_per_query)  # 0-3の関連度
    
    queries.extend([i] * n_docs_per_query)
    groups.extend([i] * n_docs_per_query)
    features.extend(query_features)
    labels.extend(query_labels)

# LightGBMでランキング学習
train_data = lgb.Dataset(
    np.array(features),
    label=np.array(labels),
    group=[n_docs_per_query] * n_queries
)

params = {
    'objective': 'lambdarank',
    'metric': 'ndcg',
    'ndcg_eval_at': [5, 10],
    'learning_rate': 0.1,
    'num_leaves': 31,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5
}

# モデル学習
model = lgb.train(params, train_data, num_boost_round=100)

print("ランキング学習完了")
```

## 3. 教師なし学習

### 3.1 クラスタリング

**クラスタリングとは？**
データを似たもの同士でグループ化する手法です。

#### 3.1.1 K-means

**K-meansとは？**
最も基本的なクラスタリング手法です。

**アルゴリズム：**
1. K個のクラスタ中心をランダムに配置
2. 各データを最も近い中心に割り当て
3. 各クラスタの中心を再計算
4. 2-3を収束するまで繰り返し

**Python実装例：**
```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# サンプルデータ（顧客の購買パターン）
np.random.seed(42)
n_samples = 300

# 3つのクラスタに分かれたデータ
cluster1 = np.random.normal([2, 2], 0.5, (n_samples//3, 2))
cluster2 = np.random.normal([8, 3], 0.5, (n_samples//3, 2))
cluster3 = np.random.normal([5, 8], 0.5, (n_samples//3, 2))

X_cluster = np.vstack([cluster1, cluster2, cluster3])

# K-meansクラスタリング
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_cluster)

# 結果の可視化
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_cluster[:, 0], X_cluster[:, 1], alpha=0.6)
plt.title('元データ')
plt.xlabel('特徴量1')
plt.ylabel('特徴量2')

plt.subplot(1, 2, 2)
plt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=clusters, alpha=0.6)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
           c='red', marker='x', s=200, linewidths=3)
plt.title('K-meansクラスタリング結果')
plt.xlabel('特徴量1')
plt.ylabel('特徴量2')

plt.tight_layout()
plt.show()

print(f"クラスタ中心:")
for i, center in enumerate(kmeans.cluster_centers_):
    print(f"クラスタ{i}: {center}")
```

#### 3.1.2 DBSCAN

**DBSCANとは？**
密度ベースのクラスタリング手法です。任意の形状のクラスタを発見できます。

**Python実装例：**
```python
from sklearn.cluster import DBSCAN

# DBSCANクラスタリング
dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters_dbscan = dbscan.fit_predict(X_cluster)

# 結果の可視化
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=clusters, alpha=0.6)
plt.title('K-means結果')

plt.subplot(1, 2, 2)
plt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=clusters_dbscan, alpha=0.6)
plt.title('DBSCAN結果')

plt.tight_layout()
plt.show()

print(f"DBSCANクラスタ数: {len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)}")
```

#### 3.1.3 Hierarchical Clustering

**Hierarchical Clusteringとは？**
階層的にクラスタを構築する手法です。

**Python実装例：**
```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# 階層的クラスタリング
hierarchical = AgglomerativeClustering(n_clusters=3)
clusters_hierarchical = hierarchical.fit_predict(X_cluster)

# デンドログラムの作成
linkage_matrix = linkage(X_cluster, method='ward')

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
dendrogram(linkage_matrix)
plt.title('デンドログラム')
plt.xlabel('サンプルインデックス')
plt.ylabel('距離')

plt.subplot(1, 2, 2)
plt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=clusters_hierarchical, alpha=0.6)
plt.title('階層的クラスタリング結果')

plt.tight_layout()
plt.show()
```

### 3.2 次元削減

**次元削減とは？**
高次元データを低次元に変換する手法です。

#### 3.2.1 PCA（Principal Component Analysis）

**PCAとは？**
データの分散を最大化する方向に射影する手法です。

**Python実装例：**
```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 高次元データの作成
np.random.seed(42)
X_high_dim = np.random.randn(100, 10)  # 100サンプル、10次元

# データの標準化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_high_dim)

# PCAによる次元削減
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 結果の可視化
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.6)
plt.title('元データ（最初の2次元）')
plt.xlabel('特徴量1')
plt.ylabel('特徴量2')

plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)
plt.title('PCA結果（2次元）')
plt.xlabel('第1主成分')
plt.ylabel('第2主成分')

plt.tight_layout()
plt.show()

print(f"説明分散比: {pca.explained_variance_ratio_}")
print(f"累積説明分散比: {np.cumsum(pca.explained_variance_ratio_)}")
```

#### 3.2.2 t-SNE

**t-SNEとは？**
高次元データの可視化に特化した次元削減手法です。

**Python実装例：**
```python
from sklearn.manifold import TSNE

# t-SNEによる次元削減
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# 結果の可視化
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)
plt.title('PCA結果')

plt.subplot(1, 2, 2)
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.6)
plt.title('t-SNE結果')

plt.tight_layout()
plt.show()
```

### 3.3 異常検知

**異常検知とは？**
正常なパターンから外れたデータを検出する手法です。

#### 3.3.1 Isolation Forest

**Isolation Forestとは？**
異常値を効率的に検出する手法です。

**Python実装例：**
```python
from sklearn.ensemble import IsolationForest

# 正常データと異常データの作成
np.random.seed(42)
normal_data = np.random.normal(0, 1, (1000, 2))
anomaly_data = np.random.normal(5, 1, (50, 2))
X_anomaly = np.vstack([normal_data, anomaly_data])

# Isolation Forestによる異常検知
iso_forest = IsolationForest(contamination=0.05, random_state=42)
anomaly_scores = iso_forest.fit_predict(X_anomaly)

# 結果の可視化
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_anomaly[:, 0], X_anomaly[:, 1], alpha=0.6)
plt.title('元データ')

plt.subplot(1, 2, 2)
colors = ['blue' if score == 1 else 'red' for score in anomaly_scores]
plt.scatter(X_anomaly[:, 0], X_anomaly[:, 1], c=colors, alpha=0.6)
plt.title('異常検知結果（赤：異常、青：正常）')

plt.tight_layout()
plt.show()

print(f"検出された異常データ数: {np.sum(anomaly_scores == -1)}")
```

## 4. 時系列分析

### 4.1 時系列予測モデル

#### 4.1.1 ARIMA

**ARIMAとは？**
自己回帰移動平均モデルです。時系列データの予測に使用されます。

**Python実装例：**
```python
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller

# 時系列データの作成
np.random.seed(42)
n_points = 200
trend = np.linspace(0, 10, n_points)
seasonal = 2 * np.sin(2 * np.pi * np.arange(n_points) / 50)
noise = np.random.normal(0, 0.5, n_points)
time_series = trend + seasonal + noise

# 定常性の検定
result = adfuller(time_series)
print(f"ADF統計量: {result[0]:.4f}")
print(f"p値: {result[1]:.4f}")

# ARIMAモデルの学習
model = ARIMA(time_series, order=(1, 1, 1))
fitted_model = model.fit()

# 予測
forecast = fitted_model.forecast(steps=20)

# 結果の可視化
plt.figure(figsize=(12, 6))
plt.plot(time_series, label='元データ')
plt.plot(range(len(time_series), len(time_series) + 20), forecast, 
         label='予測', color='red')
plt.title('ARIMA予測結果')
plt.xlabel('時間')
plt.ylabel('値')
plt.legend()
plt.show()
```

#### 4.1.2 LSTM

**LSTMとは？**
長短期記憶ネットワークです。複雑な時系列パターンを学習できます。

**Python実装例：**
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# データの準備
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:(i + seq_length)])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

# シーケンスデータの作成
seq_length = 20
X_seq, y_seq = create_sequences(time_series, seq_length)

# データの正規化
scaler = StandardScaler()
X_seq_scaled = scaler.fit_transform(X_seq.reshape(-1, X_seq.shape[-1])).reshape(X_seq.shape)
y_seq_scaled = scaler.transform(y_seq.reshape(-1, 1)).flatten()

# LSTMモデルの構築
model = Sequential([
    LSTM(50, activation='relu', input_shape=(seq_length, 1), return_sequences=True),
    LSTM(50, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')

# モデルの学習
history = model.fit(X_seq_scaled, y_seq_scaled, epochs=50, batch_size=32, verbose=0)

# 予測
predictions = model.predict(X_seq_scaled)

# 結果の可視化
plt.figure(figsize=(12, 6))
plt.plot(y_seq_scaled, label='実際の値')
plt.plot(predictions, label='予測値', color='red')
plt.title('LSTM予測結果')
plt.xlabel('時間')
plt.ylabel('値')
plt.legend()
plt.show()
```

## 5. まとめ

### 5.1 アルゴリズム選択の指針

**分類問題：**
- 小〜中規模データ：Random Forest、SVM
- 大規模データ：XGBoost、LightGBM
- 複雑なパターン：Neural Networks

**回帰問題：**
- 線形関係：Linear Regression、Ridge/Lasso
- 非線形関係：Gradient Boosting、Neural Networks

**クラスタリング：**
- 球形クラスタ：K-means
- 任意形状：DBSCAN
- 階層構造：Hierarchical Clustering

**時系列予測：**
- 単純なパターン：ARIMA
- 複雑なパターン：LSTM

### 5.2 重要なポイント

1. **データの理解**: まずはデータの性質を理解する
2. **適切なアルゴリズム選択**: 問題の種類に応じて選択
3. **ハイパーパラメータ調整**: グリッドサーチやベイズ最適化
4. **評価指標**: 適切な評価指標の選択
5. **過学習の防止**: 正則化やクロスバリデーション

### 5.3 次のステップ

1. **基本的なアルゴリズム**をマスター
2. **アンサンブル学習**を学ぶ
3. **深層学習**に挑戦
4. **実践的なプロジェクト**に取り組む

機械学習は理論と実践の両方が重要です。多くのデータセットで練習を重ねることで、自然と適切なアルゴリズムを選択できるようになります。 