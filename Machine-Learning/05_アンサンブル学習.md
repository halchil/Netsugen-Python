# アンサンブル学習 - 複数のモデルを組み合わせて性能を向上させる技術

## 1. アンサンブル学習とは？

### 1.1 アンサンブル学習の基本概念

**アンサンブル学習とは？**
複数の機械学習モデルを組み合わせて、単一のモデルよりも高い性能を達成する手法です。

**なぜ効果的なのでしょうか？**

1. **多様性**: 異なるモデルが異なるパターンを学習
2. **安定性**: 個々のモデルの過学習を軽減
3. **頑健性**: 外れ値やノイズに対する耐性
4. **精度向上**: 単一モデルを上回る性能

**例：住宅価格予測**
```
単一モデルA: RMSE = 50万円
単一モデルB: RMSE = 45万円
単一モデルC: RMSE = 48万円

アンサンブル（平均）: RMSE = 42万円
→ 個々のモデルよりも良い性能
```

### 1.2 アンサンブル学習の種類

アンサンブル学習は大きく3つに分類されます：

1. **バギング（Bootstrap Aggregating）**: 同じアルゴリズムで異なるデータサンプルを使用
2. **ブースティング**: 弱い学習器を順次追加して改善
3. **スタッキング**: 異なるアルゴリズムを組み合わせ

## 2. バギング（Bootstrap Aggregating）

### 2.1 バギングの仕組み

**バギングとは？**
同じアルゴリズムで異なるデータサンプルを使用して複数のモデルを作成し、それらの予測を平均化する手法です。

**手順：**
1. 元データからブートストラップサンプルを生成
2. 各サンプルでモデルを学習
3. 全モデルの予測を平均化

**Python実装例：**
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# サンプルデータの作成
np.random.seed(42)
n_samples = 1000

# 特徴量
X = np.random.randn(n_samples, 5)
# 目的変数（複雑な非線形関係）
y = ((X[:, 0] > 0) & (X[:, 1] > 0)) | ((X[:, 2] < 0) & (X[:, 3] < 0))
y = y.astype(int)

# データ分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 単一の決定木
single_tree = DecisionTreeClassifier(random_state=42)
single_tree.fit(X_train, y_train)
single_pred = single_tree.predict(X_test)
single_accuracy = accuracy_score(y_test, single_pred)

print(f"単一決定木の正解率: {single_accuracy:.3f}")

# バギング（Bagging）
bagging = BaggingClassifier(
    DecisionTreeClassifier(random_state=42),
    n_estimators=100,
    max_samples=0.8,
    random_state=42
)
bagging.fit(X_train, y_train)
bagging_pred = bagging.predict(X_test)
bagging_accuracy = accuracy_score(y_test, bagging_pred)

print(f"バギングの正解率: {bagging_accuracy:.3f}")
print(f"性能向上: {bagging_accuracy - single_accuracy:.3f}")
```

### 2.2 Random Forest

**Random Forestとは？**
バギングの代表的な実装で、決定木をベースとしたアンサンブル学習です。

**特徴：**
- 各決定木で異なる特徴量サブセットを使用
- 過学習の軽減
- 特徴量の重要度評価
- 欠損値の自動処理

**Python実装例：**
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Random Forestモデル
rf_model = RandomForestClassifier(
    n_estimators=100,      # 決定木の数
    max_depth=10,          # 木の最大深さ
    min_samples_split=2,   # 分割に必要な最小サンプル数
    min_samples_leaf=1,    # 葉に必要な最小サンプル数
    random_state=42
)

# モデルの学習
rf_model.fit(X_train, y_train)

# 予測
rf_pred = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)

print(f"Random Forestの正解率: {rf_accuracy:.3f}")

# 特徴量の重要度
feature_importance = pd.DataFrame({
    'feature': [f'feature_{i}' for i in range(X.shape[1])],
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n特徴量の重要度:")
print(feature_importance)

# 重要度の可視化
plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importance)), feature_importance['importance'])
plt.xlabel('特徴量')
plt.ylabel('重要度')
plt.title('Random Forest - 特徴量の重要度')
plt.xticks(range(len(feature_importance)), feature_importance['feature'], rotation=45)
plt.tight_layout()
plt.show()

# 混同行列
cm = confusion_matrix(y_test, rf_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Random Forest - 混同行列')
plt.ylabel('実際の値')
plt.xlabel('予測値')
plt.show()
```

### 2.3 Extra Trees

**Extra Treesとは？**
Random Forestの変種で、よりランダム性を高めたアンサンブル学習です。

**特徴：**
- 分割点をランダムに選択
- より高速な学習
- 過学習のさらなる軽減

**Python実装例：**
```python
from sklearn.ensemble import ExtraTreesClassifier

# Extra Treesモデル
et_model = ExtraTreesClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42
)

# モデルの学習
et_model.fit(X_train, y_train)

# 予測
et_pred = et_model.predict(X_test)
et_accuracy = accuracy_score(y_test, et_pred)

print(f"Extra Treesの正解率: {et_accuracy:.3f}")

# モデル比較
print("\nモデル比較:")
print(f"単一決定木: {single_accuracy:.3f}")
print(f"Random Forest: {rf_accuracy:.3f}")
print(f"Extra Trees: {et_accuracy:.3f}")
```

## 3. ブースティング

### 3.1 ブースティングの仕組み

**ブースティングとは？**
弱い学習器を順次追加して、前のモデルの誤りを修正していく手法です。

**手順：**
1. 最初の弱い学習器を学習
2. 誤分類されたサンプルに重みを付ける
3. 次の学習器を学習（重み付きデータで）
4. 2-3を繰り返す
5. 全学習器の予測を重み付きで結合

### 3.2 AdaBoost

**AdaBoostとは？**
最も基本的なブースティングアルゴリズムです。

**Python実装例：**
```python
from sklearn.ensemble import AdaBoostClassifier

# AdaBoostモデル
ada_model = AdaBoostClassifier(
    n_estimators=100,
    learning_rate=1.0,
    random_state=42
)

# モデルの学習
ada_model.fit(X_train, y_train)

# 予測
ada_pred = ada_model.predict(X_test)
ada_accuracy = accuracy_score(y_test, ada_pred)

print(f"AdaBoostの正解率: {ada_accuracy:.3f}")

# 学習曲線の可視化
train_scores = []
test_scores = []

for i in range(1, 101, 10):
    model = AdaBoostClassifier(n_estimators=i, random_state=42)
    model.fit(X_train, y_train)
    
    train_score = accuracy_score(y_train, model.predict(X_train))
    test_score = accuracy_score(y_test, model.predict(X_test))
    
    train_scores.append(train_score)
    test_scores.append(test_score)

plt.figure(figsize=(10, 6))
plt.plot(range(1, 101, 10), train_scores, label='訓練データ')
plt.plot(range(1, 101, 10), test_scores, label='テストデータ')
plt.xlabel('学習器の数')
plt.ylabel('正解率')
plt.title('AdaBoost - 学習曲線')
plt.legend()
plt.grid(True)
plt.show()
```

### 3.3 Gradient Boosting

**Gradient Boostingとは？**
勾配降下法を用いたブースティングアルゴリズムです。

**Python実装例：**
```python
from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boostingモデル
gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

# モデルの学習
gb_model.fit(X_train, y_train)

# 予測
gb_pred = gb_model.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)

print(f"Gradient Boostingの正解率: {gb_accuracy:.3f}")

# 特徴量の重要度
gb_importance = pd.DataFrame({
    'feature': [f'feature_{i}' for i in range(X.shape[1])],
    'importance': gb_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nGradient Boosting - 特徴量の重要度:")
print(gb_importance)
```

### 3.4 XGBoost

**XGBoostとは？**
Gradient Boostingの高性能な実装です。

**特徴：**
- 正則化機能
- 欠損値の自動処理
- 並列計算
- 早期停止機能

**Python実装例：**
```python
import xgboost as xgb

# XGBoostモデル
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# モデルの学習
xgb_model.fit(X_train, y_train)

# 予測
xgb_pred = xgb_model.predict(X_test)
xgb_accuracy = accuracy_score(y_test, xgb_pred)

print(f"XGBoostの正解率: {xgb_accuracy:.3f}")

# 特徴量の重要度
xgb_importance = pd.DataFrame({
    'feature': [f'feature_{i}' for i in range(X.shape[1])],
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nXGBoost - 特徴量の重要度:")
print(xgb_importance)

# 重要度の可視化
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.bar(range(len(feature_importance)), feature_importance['importance'])
plt.title('Random Forest - 特徴量重要度')
plt.xlabel('特徴量')
plt.ylabel('重要度')

plt.subplot(1, 2, 2)
plt.bar(range(len(xgb_importance)), xgb_importance['importance'])
plt.title('XGBoost - 特徴量重要度')
plt.xlabel('特徴量')
plt.ylabel('重要度')

plt.tight_layout()
plt.show()
```

### 3.5 LightGBM

**LightGBMとは？**
Microsoftが開発した高速なGradient Boostingライブラリです。

**特徴：**
- 高速な学習
- メモリ効率が良い
- 大規模データセットに適している

**Python実装例：**
```python
import lightgbm as lgb

# LightGBMモデル
lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# モデルの学習
lgb_model.fit(X_train, y_train)

# 予測
lgb_pred = lgb_model.predict(X_test)
lgb_accuracy = accuracy_score(y_test, lgb_pred)

print(f"LightGBMの正解率: {lgb_accuracy:.3f}")

# 特徴量の重要度
lgb_importance = pd.DataFrame({
    'feature': [f'feature_{i}' for i in range(X.shape[1])],
    'importance': lgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nLightGBM - 特徴量の重要度:")
print(lgb_importance)
```

### 3.6 CatBoost

**CatBoostとは？**
Yandexが開発したGradient Boostingライブラリです。

**特徴：**
- カテゴリカル変数の自動処理
- 過学習の軽減
- 高い予測精度

**Python実装例：**
```python
from catboost import CatBoostClassifier

# CatBoostモデル
cat_model = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=6,
    random_state=42,
    verbose=False
)

# モデルの学習
cat_model.fit(X_train, y_train)

# 予測
cat_pred = cat_model.predict(X_test)
cat_accuracy = accuracy_score(y_test, cat_pred)

print(f"CatBoostの正解率: {cat_accuracy:.3f}")

# 特徴量の重要度
cat_importance = pd.DataFrame({
    'feature': [f'feature_{i}' for i in range(X.shape[1])],
    'importance': cat_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nCatBoost - 特徴量の重要度:")
print(cat_importance)
```

## 4. スタッキング

### 4.1 スタッキングの仕組み

**スタッキングとは？**
異なるアルゴリズムの予測を組み合わせて、メタ学習器で最終予測を行う手法です。

**手順：**
1. 複数のベース学習器を学習
2. 各学習器の予測を特徴量として使用
3. メタ学習器で最終予測

**Python実装例：**
```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# ベース学習器
base_learners = [
    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
    ('svm', SVC(probability=True, random_state=42)),
    ('knn', KNeighborsClassifier(n_neighbors=5))
]

# メタ学習器
meta_learner = LogisticRegression(random_state=42)

# スタッキングモデル
stacking_model = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5
)

# モデルの学習
stacking_model.fit(X_train, y_train)

# 予測
stacking_pred = stacking_model.predict(X_test)
stacking_accuracy = accuracy_score(y_test, stacking_pred)

print(f"スタッキングの正解率: {stacking_accuracy:.3f}")

# 各ベース学習器の性能
print("\n各ベース学習器の性能:")
for name, model in base_learners:
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    acc = accuracy_score(y_test, pred)
    print(f"{name}: {acc:.3f}")

print(f"スタッキング: {stacking_accuracy:.3f}")
```

### 4.2 ブレンディング

**ブレンディングとは？**
スタッキングの簡易版で、単純な平均や重み付き平均を使用します。

**Python実装例：**
```python
class BlendingClassifier:
    """ブレンディング分類器の実装"""
    
    def __init__(self, estimators, weights=None):
        self.estimators = estimators
        self.weights = weights if weights is not None else [1/len(estimators)] * len(estimators)
    
    def fit(self, X, y):
        for estimator in self.estimators:
            estimator.fit(X, y)
        return self
    
    def predict(self, X):
        predictions = []
        for estimator in self.estimators:
            pred = estimator.predict(X)
            predictions.append(pred)
        
        # 重み付き平均
        weighted_pred = np.zeros(len(X))
        for i, weight in enumerate(self.weights):
            weighted_pred += weight * predictions[i]
        
        return (weighted_pred > 0.5).astype(int)

# ブレンディングモデル
blending_model = BlendingClassifier([
    RandomForestClassifier(n_estimators=50, random_state=42),
    SVC(probability=True, random_state=42),
    KNeighborsClassifier(n_neighbors=5)
])

# モデルの学習
blending_model.fit(X_train, y_train)

# 予測
blending_pred = blending_model.predict(X_test)
blending_accuracy = accuracy_score(y_test, blending_pred)

print(f"ブレンディングの正解率: {blending_accuracy:.3f}")
```

## 5. アンサンブル学習の比較

### 5.1 包括的な比較

**全てのアンサンブル手法の比較：**

```python
# 全てのモデルの性能比較
models = {
    'Single Tree': single_tree,
    'Random Forest': rf_model,
    'Extra Trees': et_model,
    'AdaBoost': ada_model,
    'Gradient Boosting': gb_model,
    'XGBoost': xgb_model,
    'LightGBM': lgb_model,
    'CatBoost': cat_model,
    'Stacking': stacking_model,
    'Blending': blending_model
}

# 性能比較
results = {}
for name, model in models.items():
    if hasattr(model, 'predict'):
        pred = model.predict(X_test)
        acc = accuracy_score(y_test, pred)
        results[name] = acc

# 結果の可視化
plt.figure(figsize=(12, 6))
names = list(results.keys())
accuracies = list(results.values())

plt.bar(range(len(names)), accuracies)
plt.xlabel('モデル')
plt.ylabel('正解率')
plt.title('アンサンブル学習手法の比較')
plt.xticks(range(len(names)), names, rotation=45, ha='right')
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

print("アンサンブル学習手法の性能比較:")
for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):
    print(f"{name}: {acc:.3f}")
```

### 5.2 計算時間の比較

**計算時間の比較：**

```python
import time

# 計算時間の比較
timing_results = {}

for name, model in models.items():
    if hasattr(model, 'fit'):
        start_time = time.time()
        model.fit(X_train, y_train)
        end_time = time.time()
        timing_results[name] = end_time - start_time

# 計算時間の可視化
plt.figure(figsize=(12, 6))
names = list(timing_results.keys())
times = list(timing_results.values())

plt.bar(range(len(names)), times)
plt.xlabel('モデル')
plt.ylabel('学習時間（秒）')
plt.title('アンサンブル学習手法の計算時間比較')
plt.xticks(range(len(names)), names, rotation=45, ha='right')
plt.tight_layout()
plt.show()

print("計算時間の比較:")
for name, time_taken in sorted(timing_results.items(), key=lambda x: x[1]):
    print(f"{name}: {time_taken:.3f}秒")
```

## 6. 実践的なアンサンブル学習

### 6.1 ハイパーパラメータチューニング

**アンサンブル学習のハイパーパラメータチューニング：**

```python
from sklearn.model_selection import GridSearchCV

# Random Forestのハイパーパラメータチューニング
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

rf_grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    rf_param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

rf_grid_search.fit(X_train, y_train)

print("Random Forest最適パラメータ:")
print(rf_grid_search.best_params_)
print(f"最良スコア: {rf_grid_search.best_score_:.3f}")

# XGBoostのハイパーパラメータチューニング
xgb_param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 6, 9]
}

xgb_grid_search = GridSearchCV(
    xgb.XGBClassifier(random_state=42),
    xgb_param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

xgb_grid_search.fit(X_train, y_train)

print("\nXGBoost最適パラメータ:")
print(xgb_grid_search.best_params_)
print(f"最良スコア: {xgb_grid_search.best_score_:.3f}")
```

### 6.2 特徴量選択との組み合わせ

**アンサンブル学習と特徴量選択の組み合わせ：**

```python
from sklearn.feature_selection import SelectKBest, f_classif

# 特徴量選択
selector = SelectKBest(score_func=f_classif, k=3)
X_selected = selector.fit_transform(X, y)

# 選択された特徴量
selected_features = selector.get_support()
print(f"選択された特徴量: {np.where(selected_features)[0]}")

# 特徴量選択後のアンサンブル学習
X_train_selected, X_test_selected, y_train, y_test = train_test_split(
    X_selected, y, test_size=0.2, random_state=42
)

# 特徴量選択後のRandom Forest
rf_selected = RandomForestClassifier(n_estimators=100, random_state=42)
rf_selected.fit(X_train_selected, y_train)
rf_selected_pred = rf_selected.predict(X_test_selected)
rf_selected_accuracy = accuracy_score(y_test, rf_selected_pred)

print(f"特徴量選択後のRandom Forest正解率: {rf_selected_accuracy:.3f}")
print(f"特徴量選択前: {rf_accuracy:.3f}")
print(f"性能差: {rf_selected_accuracy - rf_accuracy:.3f}")
```

## 7. まとめ

### 7.1 アンサンブル学習の選択指針

**バギング：**
- **用途**: 過学習の軽減、安定性の向上
- **代表的手法**: Random Forest、Extra Trees
- **特徴**: 並列学習可能、解釈しやすい

**ブースティング：**
- **用途**: 高精度な予測
- **代表的手法**: XGBoost、LightGBM、CatBoost
- **特徴**: 順次学習、高い予測精度

**スタッキング：**
- **用途**: 異なるアルゴリズムの組み合わせ
- **代表的手法**: Stacking、Blending
- **特徴**: 多様性の活用、メタ学習

### 7.2 重要なポイント

1. **多様性の確保**: 異なるモデルやパラメータを使用
2. **過学習の防止**: 適切な正則化とクロスバリデーション
3. **計算コスト**: 精度と計算時間のバランス
4. **解釈可能性**: ビジネス要件に応じた選択

### 7.3 次のステップ

1. **基本的なアンサンブル手法**をマスター
2. **ハイパーパラメータチューニング**の実践
3. **実践的なプロジェクト**での応用
4. **高度なアンサンブル手法**の学習

アンサンブル学習は機械学習の性能向上に非常に効果的な手法です。適切に組み合わせることで、単一モデルを大きく上回る性能を達成できます。 