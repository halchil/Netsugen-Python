# 評価指標と検証 - 機械学習モデルの性能を正しく評価する技術

## 1. 評価指標とは？

### 1.1 評価指標の重要性

**評価指標とは？**
機械学習モデルの性能を定量的に測定する指標です。

**なぜ重要なのでしょうか？**

1. **モデルの比較**: 複数のモデルを客観的に比較
2. **改善の方向性**: どの部分を改善すべきかを判断
3. **ビジネス価値**: モデルが実際に価値を生むかを評価
4. **信頼性**: モデルの信頼性を保証

**例：住宅価格予測モデル**
```
モデルA: RMSE = 50万円
モデルB: RMSE = 30万円

→ モデルBの方が良い予測性能
```

### 1.2 評価指標の種類

評価指標は問題の種類によって異なります：

1. **分類問題**: Accuracy、Precision、Recall、F1-score等
2. **回帰問題**: MSE、RMSE、MAE、R²等
3. **ランキング問題**: NDCG、MAP、MRR等

## 2. 分類問題の評価指標

### 2.1 基本的な評価指標

#### 2.1.1 Accuracy（正解率）

**Accuracyとは？**
全予測のうち、正しく予測できた割合です。

**数式：**
```
Accuracy = (正解数) / (総予測数)
```

**Python実装例：**
```python
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# サンプルデータ（スパムメール判定）
np.random.seed(42)
n_samples = 1000

# 実際の値（0: 非スパム、1: スパム）
y_true = np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3])

# 予測値（モデルA: 高精度、モデルB: 低精度）
y_pred_a = np.where(np.random.random(n_samples) < 0.9, y_true, 1 - y_true)
y_pred_b = np.random.choice([0, 1], size=n_samples)

# Accuracyの計算
accuracy_a = accuracy_score(y_true, y_pred_a)
accuracy_b = accuracy_score(y_true, y_pred_b)

print(f"モデルAのAccuracy: {accuracy_a:.3f}")
print(f"モデルBのAccuracy: {accuracy_b:.3f}")

# 混同行列の作成
def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(title)
    plt.ylabel('実際の値')
    plt.xlabel('予測値')
    plt.show()

plot_confusion_matrix(y_true, y_pred_a, 'モデルAの混同行列')
plot_confusion_matrix(y_true, y_pred_b, 'モデルBの混同行列')
```

#### 2.1.2 Precision（適合率）

**Precisionとは？**
陽性と予測したもののうち、実際に陽性だった割合です。

**数式：**
```
Precision = TP / (TP + FP)
```

**Python実装例：**
```python
from sklearn.metrics import precision_score, recall_score, f1_score

# Precisionの計算
precision_a = precision_score(y_true, y_pred_a)
precision_b = precision_score(y_true, y_pred_b)

print(f"モデルAのPrecision: {precision_a:.3f}")
print(f"モデルBのPrecision: {precision_b:.3f}")

# 例：スパムメール判定でのPrecisionの意味
print("\nPrecisionの意味（スパムメール判定）:")
print("- スパムと判定したメールのうち、実際にスパムだった割合")
print("- 高いPrecision = 誤検知が少ない")
```

#### 2.1.3 Recall（再現率）

**Recallとは？**
実際に陽性だったもののうち、陽性と予測できた割合です。

**数式：**
```
Recall = TP / (TP + FN)
```

**Python実装例：**
```python
# Recallの計算
recall_a = recall_score(y_true, y_pred_a)
recall_b = recall_score(y_true, y_pred_b)

print(f"モデルAのRecall: {recall_a:.3f}")
print(f"モデルBのRecall: {recall_b:.3f}")

# 例：スパムメール判定でのRecallの意味
print("\nRecallの意味（スパムメール判定）:")
print("- 実際のスパムメールのうち、スパムと判定できた割合")
print("- 高いRecall = 見逃しが少ない")
```

#### 2.1.4 F1-score

**F1-scoreとは？**
PrecisionとRecallの調和平均です。

**数式：**
```
F1-score = 2 × (Precision × Recall) / (Precision + Recall)
```

**Python実装例：**
```python
# F1-scoreの計算
f1_a = f1_score(y_true, y_pred_a)
f1_b = f1_score(y_true, y_pred_b)

print(f"モデルAのF1-score: {f1_a:.3f}")
print(f"モデルBのF1-score: {f1_b:.3f}")

# 総合評価
print("\n総合評価:")
print(f"モデルA - Accuracy: {accuracy_a:.3f}, Precision: {precision_a:.3f}, Recall: {recall_a:.3f}, F1: {f1_a:.3f}")
print(f"モデルB - Accuracy: {accuracy_b:.3f}, Precision: {precision_b:.3f}, Recall: {recall_b:.3f}, F1: {f1_b:.3f}")
```

### 2.2 高度な評価指標

#### 2.2.1 ROC-AUC

**ROC-AUCとは？**
受信者操作特性曲線の下の面積です。分類器の性能を評価する重要な指標です。

**Python実装例：**
```python
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# サンプルデータの作成
np.random.seed(42)
n_samples = 1000

# 特徴量
X = np.random.randn(n_samples, 3)
# 目的変数（確率的に生成）
prob = 1 / (1 + np.exp(-(X[:, 0] + 2*X[:, 1] - X[:, 2])))
y_true = (np.random.random(n_samples) < prob).astype(int)

# データ分割
X_train, X_test, y_train, y_test = train_test_split(X, y_true, test_size=0.2, random_state=42)

# モデルの学習
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# 予測確率
y_pred_proba = model.predict_proba(X_test)[:, 1]

# ROC-AUCの計算
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"ROC-AUC: {roc_auc:.3f}")

# ROC曲線の描画
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
```

#### 2.2.2 PR-AUC

**PR-AUCとは？**
Precision-Recall曲線の下の面積です。不均衡データで特に有用です。

**Python実装例：**
```python
from sklearn.metrics import average_precision_score, precision_recall_curve

# PR-AUCの計算
pr_auc = average_precision_score(y_test, y_pred_proba)
print(f"PR-AUC: {pr_auc:.3f}")

# Precision-Recall曲線の描画
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()
```

### 2.3 多クラス分類の評価指標

**多クラス分類の評価指標：**

```python
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier

# 多クラス分類のサンプルデータ
np.random.seed(42)
n_samples = 1000

# 3クラスのデータ
X_multi = np.random.randn(n_samples, 4)
y_multi = np.random.choice([0, 1, 2], size=n_samples, p=[0.4, 0.3, 0.3])

# データ分割
X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(
    X_multi, y_multi, test_size=0.2, random_state=42
)

# モデルの学習
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_multi, y_train_multi)

# 予測
y_pred_multi = rf_model.predict(X_test_multi)

# 詳細な評価レポート
print("多クラス分類の評価レポート:")
print(classification_report(y_test_multi, y_pred_multi))

# 混同行列
cm_multi = confusion_matrix(y_test_multi, y_pred_multi)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues')
plt.title('多クラス分類の混同行列')
plt.ylabel('実際の値')
plt.xlabel('予測値')
plt.show()
```

## 3. 回帰問題の評価指標

### 3.1 基本的な評価指標

#### 3.1.1 MSE（平均二乗誤差）

**MSEとは？**
予測値と実際の値の差の二乗の平均です。

**数式：**
```
MSE = (1/n) × Σ(y_i - ŷ_i)²
```

**Python実装例：**
```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression

# 回帰問題のサンプルデータ
np.random.seed(42)
n_samples = 1000

# 特徴量
X_reg = np.random.randn(n_samples, 3)
# 目的変数（線形関係 + ノイズ）
y_reg = 2*X_reg[:, 0] + 3*X_reg[:, 1] - X_reg[:, 2] + np.random.normal(0, 1, n_samples)

# データ分割
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

# モデルの学習
lr_model = LinearRegression()
lr_model.fit(X_train_reg, y_train_reg)

# 予測
y_pred_reg = lr_model.predict(X_test_reg)

# MSEの計算
mse = mean_squared_error(y_test_reg, y_pred_reg)
print(f"MSE: {mse:.3f}")

# 予測vs実際の値の可視化
plt.figure(figsize=(8, 6))
plt.scatter(y_test_reg, y_pred_reg, alpha=0.5)
plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)
plt.xlabel('実際の値')
plt.ylabel('予測値')
plt.title('予測値 vs 実際の値')
plt.show()
```

#### 3.1.2 RMSE（平均二乗誤差の平方根）

**RMSEとは？**
MSEの平方根です。元のデータと同じスケールで誤差を評価できます。

**数式：**
```
RMSE = √MSE
```

**Python実装例：**
```python
# RMSEの計算
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.3f}")

# MSEとRMSEの比較
print(f"MSE: {mse:.3f}")
print(f"RMSE: {rmse:.3f}")
print("→ RMSEは元のデータと同じスケールで誤差を評価")
```

#### 3.1.3 MAE（平均絶対誤差）

**MAEとは？**
予測値と実際の値の差の絶対値の平均です。

**数式：**
```
MAE = (1/n) × Σ|y_i - ŷ_i|
```

**Python実装例：**
```python
# MAEの計算
mae = mean_absolute_error(y_test_reg, y_pred_reg)
print(f"MAE: {mae:.3f}")

# 誤差の分布
errors = y_test_reg - y_pred_reg

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.hist(errors, bins=30, alpha=0.7)
plt.xlabel('予測誤差')
plt.ylabel('頻度')
plt.title('予測誤差の分布')

plt.subplot(1, 2, 2)
plt.scatter(y_pred_reg, errors, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('予測値')
plt.ylabel('予測誤差')
plt.title('予測値 vs 予測誤差')

plt.tight_layout()
plt.show()
```

#### 3.1.4 R²（決定係数）

**R²とは？**
モデルが説明できる分散の割合です。1に近いほど良いモデルです。

**数式：**
```
R² = 1 - (SS_res / SS_tot)
```

**Python実装例：**
```python
# R²の計算
r2 = r2_score(y_test_reg, y_pred_reg)
print(f"R²: {r2:.3f}")

# R²の解釈
print(f"\nR²の解釈:")
print(f"- R² = {r2:.3f}")
print(f"- モデルが{y_test_reg.var():.3f}の分散の{r2:.1%}を説明")
print(f"- 残りの{1-r2:.1%}は説明できていない")

# Adjusted R²の計算
n = len(y_test_reg)
p = X_test_reg.shape[1]  # 特徴量の数
adjusted_r2 = 1 - (1-r2) * (n-1) / (n-p-1)
print(f"Adjusted R²: {adjusted_r2:.3f}")
```

### 3.2 高度な回帰評価指標

#### 3.2.1 MAPE（平均絶対パーセンテージ誤差）

**MAPEとは？**
相対的な誤差を評価する指標です。

**Python実装例：**
```python
def mean_absolute_percentage_error(y_true, y_pred):
    """MAPEの計算"""
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

# MAPEの計算
mape = mean_absolute_percentage_error(y_test_reg, y_pred_reg)
print(f"MAPE: {mape:.2f}%")

# 各評価指標の比較
print("\n回帰評価指標の比較:")
print(f"MSE: {mse:.3f}")
print(f"RMSE: {rmse:.3f}")
print(f"MAE: {mae:.3f}")
print(f"R²: {r2:.3f}")
print(f"MAPE: {mape:.2f}%")
```

## 4. ランキング問題の評価指標

### 4.1 基本的なランキング評価指標

#### 4.1.1 NDCG（Normalized Discounted Cumulative Gain）

**NDCGとは？**
ランキングの品質を評価する指標です。上位のアイテムにより重みを置きます。

**Python実装例：**
```python
from sklearn.metrics import ndcg_score

# ランキング問題のサンプルデータ
np.random.seed(42)
n_queries = 100
n_docs_per_query = 10

# クエリごとの関連度スコア
relevance_scores = []
predicted_scores = []

for i in range(n_queries):
    # 実際の関連度（0-4のスコア）
    true_relevance = np.random.randint(0, 5, n_docs_per_query)
    # 予測スコア
    predicted_score = np.random.randn(n_docs_per_query)
    
    relevance_scores.append(true_relevance)
    predicted_scores.append(predicted_score)

# NDCGの計算
ndcg = ndcg_score(relevance_scores, predicted_scores)
print(f"NDCG: {ndcg:.3f}")

# 上位k件でのNDCG
ndcg_at_5 = ndcg_score(relevance_scores, predicted_scores, k=5)
ndcg_at_10 = ndcg_score(relevance_scores, predicted_scores, k=10)
print(f"NDCG@5: {ndcg_at_5:.3f}")
print(f"NDCG@10: {ndcg_at_10:.3f}")
```

#### 4.1.2 MAP（Mean Average Precision）

**MAPとは？**
平均適合率の平均です。ランキングの精度を評価します。

**Python実装例：**
```python
def average_precision_at_k(y_true, y_pred, k):
    """k件での平均適合率を計算"""
    # 上位k件のインデックス
    top_k_indices = np.argsort(y_pred)[::-1][:k]
    
    # 関連アイテムの数
    relevant_count = 0
    precision_sum = 0
    
    for i, idx in enumerate(top_k_indices):
        if y_true[idx] > 0:  # 関連アイテム
            relevant_count += 1
            precision_at_k = relevant_count / (i + 1)
            precision_sum += precision_at_k
    
    return precision_sum / relevant_count if relevant_count > 0 else 0

def mean_average_precision(y_true_list, y_pred_list, k):
    """MAP@kの計算"""
    aps = []
    for y_true, y_pred in zip(y_true_list, y_pred_list):
        ap = average_precision_at_k(y_true, y_pred, k)
        aps.append(ap)
    return np.mean(aps)

# MAPの計算
map_at_5 = mean_average_precision(relevance_scores, predicted_scores, 5)
map_at_10 = mean_average_precision(relevance_scores, predicted_scores, 10)
print(f"MAP@5: {map_at_5:.3f}")
print(f"MAP@10: {map_at_10:.3f}")
```

#### 4.1.3 MRR（Mean Reciprocal Rank）

**MRRとは？**
最初の関連アイテムの順位の逆数の平均です。

**Python実装例：**
```python
def reciprocal_rank(y_true, y_pred):
    """逆順位を計算"""
    # 関連アイテムのインデックス
    relevant_indices = np.where(y_true > 0)[0]
    
    if len(relevant_indices) == 0:
        return 0
    
    # 予測スコアでソート
    sorted_indices = np.argsort(y_pred)[::-1]
    
    # 最初の関連アイテムの順位を探す
    for rank, idx in enumerate(sorted_indices, 1):
        if idx in relevant_indices:
            return 1.0 / rank
    
    return 0

def mean_reciprocal_rank(y_true_list, y_pred_list):
    """MRRの計算"""
    rrs = []
    for y_true, y_pred in zip(y_true_list, y_pred_list):
        rr = reciprocal_rank(y_true, y_pred)
        rrs.append(rr)
    return np.mean(rrs)

# MRRの計算
mrr = mean_reciprocal_rank(relevance_scores, predicted_scores)
print(f"MRR: {mrr:.3f}")
```

### 4.2 ランキング評価の可視化

**ランキング評価の可視化：**

```python
# ランキング評価の可視化
plt.figure(figsize=(15, 5))

# NDCG@kの可視化
k_values = [1, 3, 5, 10]
ndcg_values = [ndcg_score(relevance_scores, predicted_scores, k=k) for k in k_values]

plt.subplot(1, 3, 1)
plt.plot(k_values, ndcg_values, 'o-')
plt.xlabel('k')
plt.ylabel('NDCG@k')
plt.title('NDCG@k')

# MAP@kの可視化
map_values = [mean_average_precision(relevance_scores, predicted_scores, k) for k in k_values]

plt.subplot(1, 3, 2)
plt.plot(k_values, map_values, 'o-')
plt.xlabel('k')
plt.ylabel('MAP@k')
plt.title('MAP@k')

# 関連度分布の可視化
all_relevance = np.concatenate(relevance_scores)
plt.subplot(1, 3, 3)
plt.hist(all_relevance, bins=5, alpha=0.7)
plt.xlabel('関連度')
plt.ylabel('頻度')
plt.title('関連度の分布')

plt.tight_layout()
plt.show()
```

## 5. クロスバリデーション

### 5.1 クロスバリデーションとは？

**クロスバリデーションとは？**
データを複数の分割で学習・評価を行い、モデルの汎化性能を評価する手法です。

**目的：**
1. **過学習の検出**: 訓練データでの性能とテストデータでの性能の差
2. **ハイパーパラメータの選択**: 異なる設定での性能比較
3. **信頼性の向上**: より安定した性能評価

### 5.2 k分割クロスバリデーション

**k分割クロスバリデーションの手順：**

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier

# サンプルデータ
np.random.seed(42)
X_cv = np.random.randn(1000, 5)
y_cv = np.random.choice([0, 1], size=1000, p=[0.7, 0.3])

# 5分割クロスバリデーション
cv = KFold(n_splits=5, shuffle=True, random_state=42)
model_cv = RandomForestClassifier(n_estimators=100, random_state=42)

# クロスバリデーションスコアの計算
cv_scores = cross_val_score(model_cv, X_cv, y_cv, cv=cv, scoring='accuracy')

print("クロスバリデーション結果:")
print(f"各分割のスコア: {cv_scores}")
print(f"平均スコア: {cv_scores.mean():.3f}")
print(f"標準偏差: {cv_scores.std():.3f}")
print(f"95%信頼区間: [{cv_scores.mean() - 1.96*cv_scores.std():.3f}, {cv_scores.mean() + 1.96*cv_scores.std():.3f}]")

# 結果の可視化
plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.plot(range(1, 6), cv_scores, 'o-')
plt.xlabel('分割')
plt.ylabel('Accuracy')
plt.title('各分割での性能')

plt.subplot(1, 2, 2)
plt.hist(cv_scores, bins=10, alpha=0.7)
plt.xlabel('Accuracy')
plt.ylabel('頻度')
plt.title('性能の分布')

plt.tight_layout()
plt.show()
```

### 5.3 層化k分割クロスバリデーション

**層化k分割クロスバリデーションとは？**
各分割でクラスの比率を保つクロスバリデーションです。

```python
from sklearn.model_selection import StratifiedKFold

# 層化k分割クロスバリデーション
stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 層化クロスバリデーションスコアの計算
stratified_scores = cross_val_score(model_cv, X_cv, y_cv, cv=stratified_cv, scoring='accuracy')

print("層化クロスバリデーション結果:")
print(f"各分割のスコア: {stratified_scores}")
print(f"平均スコア: {stratified_scores.mean():.3f}")
print(f"標準偏差: {stratified_scores.std():.3f}")

# 通常のクロスバリデーションとの比較
print(f"\n比較:")
print(f"通常のCV: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}")
print(f"層化CV: {stratified_scores.mean():.3f} ± {stratified_scores.std():.3f}")
```

### 5.4 時系列データのクロスバリデーション

**時系列データのクロスバリデーション：**

```python
from sklearn.model_selection import TimeSeriesSplit

# 時系列データの作成
np.random.seed(42)
n_samples = 1000
time_series_data = np.random.randn(n_samples, 3)
time_series_target = np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3])

# 時系列クロスバリデーション
tscv = TimeSeriesSplit(n_splits=5)

# 時系列クロスバリデーションスコアの計算
ts_scores = cross_val_score(model_cv, time_series_data, time_series_target, cv=tscv, scoring='accuracy')

print("時系列クロスバリデーション結果:")
print(f"各分割のスコア: {ts_scores}")
print(f"平均スコア: {ts_scores.mean():.3f}")
print(f"標準偏差: {ts_scores.std():.3f}")

# 時系列分割の可視化
plt.figure(figsize=(12, 4))
for i, (train_idx, test_idx) in enumerate(tscv.split(time_series_data)):
    plt.subplot(1, 5, i+1)
    plt.plot(train_idx, [1]*len(train_idx), 'b-', label='訓練データ')
    plt.plot(test_idx, [1]*len(test_idx), 'r-', label='テストデータ')
    plt.title(f'分割 {i+1}')
    plt.ylim(0, 2)
    if i == 0:
        plt.legend()

plt.tight_layout()
plt.show()
```

## 6. ハイパーパラメータチューニング

### 6.1 グリッドサーチ

**グリッドサーチとは？**
指定したパラメータの組み合わせを全て試す手法です。

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

# グリッドサーチのパラメータ
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1],
    'kernel': ['rbf', 'linear']
}

# SVMモデル
svm_model = SVC(random_state=42)

# グリッドサーチ
grid_search = GridSearchCV(
    svm_model, 
    param_grid, 
    cv=5, 
    scoring='accuracy',
    n_jobs=-1
)

# グリッドサーチの実行
grid_search.fit(X_cv, y_cv)

print("グリッドサーチ結果:")
print(f"最良のパラメータ: {grid_search.best_params_}")
print(f"最良のスコア: {grid_search.best_score_:.3f}")

# 結果の可視化
results = grid_search.cv_results_
C_values = [0.1, 1, 10, 100]
gamma_values = [0.001, 0.01, 0.1, 1]

# RBFカーネルの結果を可視化
rbf_scores = results['mean_test_score'][results['param_kernel'] == 'rbf'].reshape(4, 4)

plt.figure(figsize=(8, 6))
plt.imshow(rbf_scores, cmap='viridis', aspect='auto')
plt.colorbar(label='Accuracy')
plt.xticks(range(4), gamma_values)
plt.yticks(range(4), C_values)
plt.xlabel('gamma')
plt.ylabel('C')
plt.title('SVM RBF Kernel - Grid Search Results')
plt.show()
```

### 6.2 ランダムサーチ

**ランダムサーチとは？**
指定した範囲からランダムにパラメータを選択する手法です。

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# ランダムサーチのパラメータ分布
param_distributions = {
    'C': uniform(0.1, 100),
    'gamma': uniform(0.001, 1),
    'kernel': ['rbf', 'linear']
}

# ランダムサーチ
random_search = RandomizedSearchCV(
    svm_model,
    param_distributions,
    n_iter=20,  # 試行回数
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

# ランダムサーチの実行
random_search.fit(X_cv, y_cv)

print("ランダムサーチ結果:")
print(f"最良のパラメータ: {random_search.best_params_}")
print(f"最良のスコア: {random_search.best_score_:.3f}")

# グリッドサーチとの比較
print(f"\n比較:")
print(f"グリッドサーチ: {grid_search.best_score_:.3f}")
print(f"ランダムサーチ: {random_search.best_score_:.3f}")
```

### 6.3 ベイズ最適化

**ベイズ最適化とは？**
過去の結果を利用して効率的にパラメータを探索する手法です。

```python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern
from scipy.optimize import minimize

# ベイズ最適化の簡易実装
def objective_function(params):
    """目的関数（負の精度を返す）"""
    C, gamma = params
    model = SVC(C=C, gamma=gamma, random_state=42)
    scores = cross_val_score(model, X_cv, y_cv, cv=5, scoring='accuracy')
    return -scores.mean()

# 初期点での評価
initial_points = [
    [1.0, 0.1],
    [10.0, 0.01],
    [0.1, 1.0]
]

results = []
for point in initial_points:
    score = objective_function(point)
    results.append({'params': point, 'score': score})

print("ベイズ最適化の初期評価:")
for i, result in enumerate(results):
    print(f"試行{i+1}: C={result['params'][0]:.3f}, gamma={result['params'][1]:.3f}, score={-result['score']:.3f}")

# 最適化の実行
bounds = [(0.1, 100), (0.001, 1)]
result = minimize(objective_function, [1.0, 0.1], bounds=bounds, method='L-BFGS-B')

print(f"\nベイズ最適化結果:")
print(f"最良のパラメータ: C={result.x[0]:.3f}, gamma={result.x[1]:.3f}")
print(f"最良のスコア: {-result.fun:.3f}")
```

## 7. まとめ

### 7.1 評価指標選択の指針

**分類問題：**
- **不均衡データ**: Precision、Recall、F1-score、PR-AUC
- **バランスデータ**: Accuracy、ROC-AUC
- **多クラス**: マクロ平均、マイクロ平均

**回帰問題：**
- **外れ値に敏感**: RMSE
- **外れ値に頑健**: MAE
- **相対的評価**: MAPE、R²

**ランキング問題：**
- **上位重視**: NDCG@k、MAP@k
- **順位重視**: MRR

### 7.2 重要なポイント

1. **適切な指標選択**: 問題の性質に応じた指標選択
2. **複数指標の併用**: 単一指標に依存しない
3. **クロスバリデーション**: 安定した性能評価
4. **ハイパーパラメータチューニング**: 系統的な最適化

### 7.3 次のステップ

1. **基本的な評価指標**をマスター
2. **クロスバリデーション**の理解
3. **ハイパーパラメータチューニング**の実践
4. **実践的なプロジェクト**での応用

評価指標と検証は機械学習の成功を左右する重要な技術です。適切な評価を行うことで、信頼性の高いモデルを構築できます。 