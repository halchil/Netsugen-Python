
## 1. 準備

### 1.1 仮想環境の作成とパッケージインストール

```bash
python -m venv venv
source venv/bin/activate  # Windowsの場合は venv\Scripts\activate
pip install requests beautifulsoup4 pandas matplotlib wordcloud mecab-python3 nltk
```

---

## 2. サンプルWebサイトからのデータ取得

### 2.1 requestsとBeautifulSoupでニュース記事を取得

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://www3.nhk.or.jp/news/catnew.html"  # サンプル: NHKニュース（実際は適宜変更）
headers = {'User-Agent': 'Mozilla/5.0'}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')

# 記事タイトルとリンクの抽出
articles = []
for a in soup.find_all('a', class_='content--list-item'):  # 適宜クラス名を調整
    title = a.get_text(strip=True)
    link = a.get('href')
    if link and not link.startswith('http'):
        link = 'https://www3.nhk.or.jp' + link
    articles.append({'title': title, 'link': link})

# DataFrame化
news_df = pd.DataFrame(articles)
print(news_df.head())
```

---

## 3. 形態素解析（日本語テキスト）

### 3.1 MeCabで記事タイトルを分かち書き

```python
import MeCab

mecab = MeCab.Tagger('-Owakati')
news_df['wakati'] = news_df['title'].apply(lambda x: mecab.parse(x).strip())
print(news_df[['title', 'wakati']].head())
```

---

## 4. 頻出語・ワードクラウドの作成

### 4.1 頻出単語の集計

```python
from collections import Counter
all_words = ' '.join(news_df['wakati']).split()
word_counts = Counter(all_words)
print(word_counts.most_common(20))
```

### 4.2 ワードクラウドの作成

```python
from wordcloud import WordCloud
import matplotlib.pyplot as plt

wordcloud = WordCloud(font_path='C:/Windows/Fonts/YuGothM.ttc', width=800, height=400, background_color='white').generate(' '.join(all_words))
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

---

## 5. 感情分析の実践（簡易版）

### 5.1 ポジティブ・ネガティブ単語リストによるスコアリング

```python
positive_words = ['良い', '素晴らしい', '最高', '嬉しい', '成功']
negative_words = ['悪い', '最悪', '失敗', '悲しい', '問題']

def sentiment_score(text):
    score = 0
    for w in positive_words:
        if w in text:
            score += 1
    for w in negative_words:
        if w in text:
            score -= 1
    return score

news_df['sentiment'] = news_df['title'].apply(sentiment_score)
print(news_df[['title', 'sentiment']].head())
```

---

## 6. 可視化

### 6.1 感情スコアのヒストグラム

```python
plt.hist(news_df['sentiment'], bins=range(-3, 4), color='skyblue', edgecolor='black')
plt.title('記事タイトルの感情スコア分布')
plt.xlabel('感情スコア')
plt.ylabel('記事数')
plt.show()
```

---

## 7. まとめ

- Webスクレイピングで外部データを自動取得する方法を学びました。
- 形態素解析で日本語テキストを分かち書きし、頻出語やワードクラウドで可視化しました。
- シンプルな感情分析を実装し、記事タイトルの傾向を把握しました。
- これらの技術は、SNS分析や顧客の声の分析など、さまざまなビジネス応用に役立ちます。
