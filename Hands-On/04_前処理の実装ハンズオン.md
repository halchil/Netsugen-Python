
## 実践演習：データクレンジングと前処理

### 演習の準備

まず、必要なライブラリをインポートし、実際のデータに近いサンプルデータを作成しましょう。

```python
# 必要なライブラリのインポート
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# 日本語フォントの設定
plt.rcParams['font.family'] = 'DejaVu Sans'

# 再現性のための乱数設定
np.random.seed(42)

# 実際のデータに近い、様々な問題を含むサンプルデータを作成
def create_sample_data():
    """実際のデータに近い、様々な問題を含むサンプルデータを作成"""
    
    # 基本データの生成
    n_samples = 1000
    
    # 正常なデータ
    normal_data = {
        'id': range(1, n_samples + 1),
        'name': [f'従業員{i:04d}' for i in range(1, n_samples + 1)],
        'age': np.random.normal(35, 8, n_samples).astype(int),
        'salary': np.random.normal(400000, 80000, n_samples).astype(int),
        'department': np.random.choice(['営業部', '開発部', '人事部', '経理部', 'マーケティング部'], n_samples),
        'years_of_service': np.random.exponential(5, n_samples).astype(int),
        'performance_score': np.random.normal(75, 15, n_samples).round(1),
        'email': [f'employee{i:04d}@company.com' for i in range(1, n_samples + 1)]
    }
    
    df = pd.DataFrame(normal_data)
    
    # 意図的に問題のあるデータを追加
    
    # 1. 欠損値の追加
    df.loc[100:120, 'age'] = np.nan
    df.loc[200:220, 'salary'] = np.nan
    df.loc[300:320, 'department'] = np.nan
    df.loc[400:420, 'performance_score'] = np.nan
    df.loc[500:520, 'email'] = np.nan
    
    # 2. 外れ値の追加
    df.loc[600, 'age'] = 150  # 異常な年齢
    df.loc[601, 'salary'] = 50000000  # 異常な給与
    df.loc[602, 'performance_score'] = 200  # 異常なスコア
    
    # 3. 重複データの追加
    df.loc[700:710] = df.loc[100:110].values
    
    # 4. 不整合なデータ型の追加
    df.loc[800:810, 'age'] = ['25歳', '30歳', '35歳', '40歳', '45歳', '50歳', '55歳', '60歳', '65歳', '70歳', '75歳']
    df.loc[900:910, 'salary'] = ['300,000', '350,000', '400,000', '450,000', '500,000', '550,000', '600,000', '650,000', '700,000', '750,000', '800,000']
    
    # 5. 異常な文字列データ
    df.loc[950:960, 'name'] = ['', '   ', 'NULL', 'N/A', 'undefined', 'test', 'sample', 'dummy', 'temp', 'unknown', 'error']
    
    return df

# サンプルデータの作成
df = create_sample_data()

print("=== 元データの基本情報 ===")
print(f"データ形状: {df.shape}")
print(f"列名: {df.columns.tolist()}")
print("\nデータ型:")
print(df.dtypes)
print("\n最初の10行:")
print(df.head(10))
```

### 演習1：データの探索と問題の特定

#### 1.1 基本的なデータ確認

```python
print("=== データの探索と問題の特定 ===")

# 基本統計情報
print("基本統計情報:")
print(df.describe())

# 欠損値の確認
print("\n欠損値の確認:")
missing_info = df.isnull().sum()
missing_percentage = (missing_info / len(df)) * 100
missing_df = pd.DataFrame({
    '欠損数': missing_info,
    '欠損率(%)': missing_percentage
})
print(missing_df[missing_df['欠損数'] > 0])

# データ型の確認
print("\nデータ型の確認:")
print(df.dtypes)

# 各列のユニーク値数
print("\n各列のユニーク値数:")
print(df.nunique())

# 重複データの確認
print(f"\n重複データの数: {df.duplicated().sum()}")
```

#### 1.2 データの可視化による問題の特定

```python
print("\n=== データの可視化による問題の特定 ===")

# 図のサイズを設定
plt.figure(figsize=(20, 15))

# 1. 年齢分布の確認
plt.subplot(3, 3, 1)
plt.hist(df['age'].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('年齢分布')
plt.xlabel('年齢')
plt.ylabel('人数')

# 2. 給与分布の確認
plt.subplot(3, 3, 2)
plt.hist(df['salary'].dropna(), bins=30, alpha=0.7, color='lightgreen', edgecolor='black')
plt.title('給与分布')
plt.xlabel('給与（円）')
plt.ylabel('人数')

# 3. パフォーマンススコア分布の確認
plt.subplot(3, 3, 3)
plt.hist(df['performance_score'].dropna(), bins=30, alpha=0.7, color='orange', edgecolor='black')
plt.title('パフォーマンススコア分布')
plt.xlabel('スコア')
plt.ylabel('人数')

# 4. 部署別の従業員数
plt.subplot(3, 3, 4)
dept_counts = df['department'].value_counts()
dept_counts.plot(kind='bar', color='purple', alpha=0.7)
plt.title('部署別従業員数')
plt.xlabel('部署')
plt.ylabel('人数')
plt.xticks(rotation=45)

# 5. 年齢と給与の散布図
plt.subplot(3, 3, 5)
plt.scatter(df['age'], df['salary'], alpha=0.6, color='red')
plt.title('年齢と給与の関係')
plt.xlabel('年齢')
plt.ylabel('給与（円）')

# 6. 勤続年数分布
plt.subplot(3, 3, 6)
plt.hist(df['years_of_service'].dropna(), bins=20, alpha=0.7, color='gold', edgecolor='black')
plt.title('勤続年数分布')
plt.xlabel('勤続年数')
plt.ylabel('人数')

# 7. 欠損値の可視化
plt.subplot(3, 3, 7)
missing_data = df.isnull().sum()
missing_data.plot(kind='bar', color='red', alpha=0.7)
plt.title('欠損値の数')
plt.xlabel('列名')
plt.ylabel('欠損数')
plt.xticks(rotation=45)

# 8. データ型の分布
plt.subplot(3, 3, 8)
data_types = df.dtypes.value_counts()
data_types.plot(kind='pie', autopct='%1.1f%%')
plt.title('データ型の分布')

# 9. 外れ値の箱ひげ図（年齢）
plt.subplot(3, 3, 9)
plt.boxplot(df['age'].dropna())
plt.title('年齢の箱ひげ図')
plt.ylabel('年齢')

plt.tight_layout()
plt.show()
```

### 演習2：欠損値の処理

#### 2.1 欠損値の詳細分析

```python
print("=== 欠損値の詳細分析 ===")

# 欠損値の詳細確認
print("欠損値の詳細:")
for column in df.columns:
    missing_count = df[column].isnull().sum()
    if missing_count > 0:
        print(f"{column}: {missing_count}個 ({missing_count/len(df)*100:.1f}%)")

# 欠損値のパターン分析
print("\n欠損値のパターン分析:")
missing_pattern = df.isnull().sum(axis=1)
print(f"欠損値を含む行数: {(missing_pattern > 0).sum()}")
print(f"複数の欠損値を含む行数: {(missing_pattern > 1).sum()}")

# 欠損値の相関分析
print("\n欠損値の相関分析:")
missing_corr = df.isnull().corr()
print(missing_corr)
```

#### 2.2 欠損値の処理方法の選択

```python
print("\n=== 欠損値の処理方法の選択 ===")

# 各列の特性に基づく処理方法の決定
def determine_missing_strategy(df, column):
    """各列の特性に基づいて欠損値の処理方法を決定"""
    
    missing_count = df[column].isnull().sum()
    missing_rate = missing_count / len(df)
    
    print(f"\n{column}の分析:")
    print(f"  欠損数: {missing_count}")
    print(f"  欠損率: {missing_rate:.1%}")
    
    if missing_rate > 0.5:
        print(f"  推奨方法: 列の削除（欠損率が50%を超える）")
        return 'drop_column'
    elif missing_rate > 0.1:
        print(f"  推奨方法: 高度な補完（回帰、KNN等）")
        return 'advanced_imputation'
    else:
        if df[column].dtype in ['int64', 'float64']:
            print(f"  推奨方法: 統計的補完（平均値、中央値等）")
            return 'statistical_imputation'
        else:
            print(f"  推奨方法: 最頻値補完")
            return 'mode_imputation'

# 各列の処理方法を決定
strategies = {}
for column in df.columns:
    if df[column].isnull().sum() > 0:
        strategies[column] = determine_missing_strategy(df, column)
```

#### 2.3 欠損値の処理実装

```python
print("\n=== 欠損値の処理実装 ===")

# 処理前のデータをコピー
df_cleaned = df.copy()

# 1. 統計的補完（数値変数）
numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns
for col in numeric_columns:
    if df_cleaned[col].isnull().sum() > 0:
        # 中央値で補完
        median_value = df_cleaned[col].median()
        df_cleaned[col].fillna(median_value, inplace=True)
        print(f"{col}: 中央値 {median_value:.2f} で補完")

# 2. 最頻値補完（カテゴリ変数）
categorical_columns = df_cleaned.select_dtypes(include=['object']).columns
for col in categorical_columns:
    if df_cleaned[col].isnull().sum() > 0:
        # 最頻値で補完
        mode_value = df_cleaned[col].mode()[0]
        df_cleaned[col].fillna(mode_value, inplace=True)
        print(f"{col}: 最頻値 '{mode_value}' で補完")

# 3. 高度な補完（グループ別平均）
# 部署別の平均年齢で補完
dept_age_mean = df_cleaned.groupby('department')['age'].mean()
for dept in df_cleaned['department'].unique():
    if pd.notna(dept):
        mask = (df_cleaned['department'] == dept) & df_cleaned['age'].isnull()
        if mask.sum() > 0:
            df_cleaned.loc[mask, 'age'] = dept_age_mean[dept]
            print(f"部署 '{dept}' の平均年齢 {dept_age_mean[dept]:.1f} で補完")

# 処理後の確認
print("\n欠損値処理後の確認:")
print(df_cleaned.isnull().sum())
```

### 演習3：外れ値の検出と処理

#### 3.1 外れ値の検出

```python
print("=== 外れ値の検出 ===")

# 外れ値検出関数の定義
def detect_outliers_iqr(data):
    """IQR法による外れ値検出"""
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (data < lower_bound) | (data > upper_bound)

def detect_outliers_zscore(data, threshold=3):
    """Z-score法による外れ値検出"""
    z_scores = np.abs((data - data.mean()) / data.std())
    return z_scores > threshold

# 数値列の外れ値検出
numeric_columns = ['age', 'salary', 'performance_score', 'years_of_service']
outlier_results = {}

for col in numeric_columns:
    if col in df_cleaned.columns:
        outliers_iqr = detect_outliers_iqr(df_cleaned[col])
        outliers_zscore = detect_outliers_zscore(df_cleaned[col])
        
        outlier_results[col] = {
            'IQR法': outliers_iqr.sum(),
            'Z-score法': outliers_zscore.sum(),
            'IQR法の割合': f"{outliers_iqr.sum()/len(df_cleaned)*100:.1f}%",
            'Z-score法の割合': f"{outliers_zscore.sum()/len(df_cleaned)*100:.1f}%"
        }
        
        print(f"\n{col}の外れ値検出結果:")
        print(f"  IQR法: {outliers_iqr.sum()}個 ({outliers_iqr.sum()/len(df_cleaned)*100:.1f}%)")
        print(f"  Z-score法: {outliers_zscore.sum()}個 ({outliers_zscore.sum()/len(df_cleaned)*100:.1f}%)")

# 外れ値の可視化
plt.figure(figsize=(15, 10))

for i, col in enumerate(numeric_columns, 1):
    if col in df_cleaned.columns:
        plt.subplot(2, 2, i)
        plt.boxplot(df_cleaned[col].dropna())
        plt.title(f'{col}の箱ひげ図')
        plt.ylabel(col)

plt.tight_layout()
plt.show()
```

#### 3.2 外れ値の処理

```python
print("\n=== 外れ値の処理 ===")

# 外れ値処理関数の定義
def cap_outliers(df, column, method='iqr'):
    """外れ値を上限・下限値で置換"""
    df_capped = df.copy()
    
    if method == 'iqr':
        Q1 = df_capped[column].quantile(0.25)
        Q3 = df_capped[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
    elif method == 'zscore':
        mean = df_capped[column].mean()
        std = df_capped[column].std()
        lower_bound = mean - 3 * std
        upper_bound = mean + 3 * std
    
    # 外れ値を制限
    df_capped[column] = df_capped[column].clip(lower=lower_bound, upper=upper_bound)
    
    return df_capped, lower_bound, upper_bound

# 各数値列の外れ値を処理
for col in numeric_columns:
    if col in df_cleaned.columns:
        df_cleaned, lower_bound, upper_bound = cap_outliers(df_cleaned, col, 'iqr')
        print(f"{col}: 下限 {lower_bound:.2f}, 上限 {upper_bound:.2f} で制限")

# 処理後の確認
print("\n外れ値処理後の統計:")
print(df_cleaned[numeric_columns].describe())
```

### 演習4：データ型の変換と正規化

#### 4.1 データ型の変換

```python
print("=== データ型の変換 ===")

# 文字列データの数値変換
def clean_numeric_string(df, column):
    """文字列の数値データを数値型に変換"""
    df_cleaned = df.copy()
    
    # カンマや通貨記号を除去
    if df_cleaned[column].dtype == 'object':
        df_cleaned[column] = df_cleaned[column].astype(str).str.replace(',', '').str.replace('¥', '')
        df_cleaned[column] = df_cleaned[column].str.replace('歳', '')
        
        # 数値に変換
        df_cleaned[column] = pd.to_numeric(df_cleaned[column], errors='coerce')
    
    return df_cleaned

# 年齢と給与の文字列データを数値に変換
df_cleaned = clean_numeric_string(df_cleaned, 'age')
df_cleaned = clean_numeric_string(df_cleaned, 'salary')

# カテゴリ変数の処理
def clean_categorical_data(df, column):
    """カテゴリ変数のクリーニング"""
    df_cleaned = df.copy()
    
    # 空文字列や特殊値をNaNに変換
    df_cleaned[column] = df_cleaned[column].replace(['', '   ', 'NULL', 'N/A', 'undefined', 'test', 'sample', 'dummy', 'temp', 'unknown', 'error'], np.nan)
    
    # 最頻値で補完
    if df_cleaned[column].isnull().sum() > 0:
        mode_value = df_cleaned[column].mode()[0]
        df_cleaned[column].fillna(mode_value, inplace=True)
    
    # カテゴリ型に変換
    df_cleaned[column] = df_cleaned[column].astype('category')
    
    return df_cleaned

# カテゴリ変数のクリーニング
categorical_columns = ['department', 'name']
for col in categorical_columns:
    if col in df_cleaned.columns:
        df_cleaned = clean_categorical_data(df_cleaned, col)

# データ型の確認
print("データ型変換後の確認:")
print(df_cleaned.dtypes)
```

#### 4.2 正規化と標準化

```python
print("\n=== 正規化と標準化 ===")

# 数値列の選択
numeric_columns = ['age', 'salary', 'performance_score', 'years_of_service']
numeric_data = df_cleaned[numeric_columns].copy()

# 1. 標準化（Z-score正規化）
scaler_standard = StandardScaler()
data_standardized = scaler_standard.fit_transform(numeric_data)
df_standardized = pd.DataFrame(data_standardized, columns=numeric_columns, index=df_cleaned.index)

# 2. 正規化（Min-Max正規化）
scaler_minmax = MinMaxScaler()
data_normalized = scaler_minmax.fit_transform(numeric_data)
df_normalized = pd.DataFrame(data_normalized, columns=numeric_columns, index=df_cleaned.index)

# 3. ロバストスケーリング
from sklearn.preprocessing import RobustScaler
scaler_robust = RobustScaler()
data_robust = scaler_robust.fit_transform(numeric_data)
df_robust = pd.DataFrame(data_robust, columns=numeric_columns, index=df_cleaned.index)

# 結果の比較
print("元データの統計:")
print(numeric_data.describe())

print("\n標準化後の統計:")
print(df_standardized.describe())

print("\n正規化後の統計:")
print(df_normalized.describe())

print("\nロバストスケーリング後の統計:")
print(df_robust.describe())
```

### 演習5：重複データの処理

#### 5.1 重複データの検出と分析

```python
print("=== 重複データの処理 ===")

# 重複データの検出
duplicates = df_cleaned.duplicated()
print(f"完全重複データの数: {duplicates.sum()}")

# 特定の列での重複検出
duplicates_subset = df_cleaned.duplicated(subset=['name', 'email'])
print(f"名前とメールでの重複データ数: {duplicates_subset.sum()}")

# 重複データの詳細確認
if duplicates.sum() > 0:
    duplicate_rows = df_cleaned[df_cleaned.duplicated(keep=False)]
    print("\n重複データの詳細:")
    print(duplicate_rows[['name', 'age', 'department', 'salary']].head(10))

# 重複データの削除
df_cleaned = df_cleaned.drop_duplicates()
print(f"\n重複削除後のデータ数: {len(df_cleaned)}")

# 特定の列を基準とした重複削除
df_cleaned = df_cleaned.drop_duplicates(subset=['name', 'email'])
print(f"名前とメールでの重複削除後のデータ数: {len(df_cleaned)}")
```

### 演習6：データの一貫性チェック

#### 6.1 データの整合性確認

```python
print("=== データの整合性チェック ===")

# データの範囲チェック
def check_data_ranges(df):
    """データの範囲チェック"""
    range_checks = {
        'age': (18, 70),
        'salary': (200000, 10000000),
        'performance_score': (0, 100),
        'years_of_service': (0, 50)
    }
    
    violations = {}
    for column, (min_val, max_val) in range_checks.items():
        if column in df.columns:
            violations[column] = df[(df[column] < min_val) | (df[column] > max_val)]
            if len(violations[column]) > 0:
                print(f"{column}: {len(violations[column])}個の範囲外データ")
    
    return violations

# 範囲チェックの実行
range_violations = check_data_ranges(df_cleaned)

# 論理的な整合性チェック
def check_logical_consistency(df):
    """論理的な整合性チェック"""
    violations = {}
    
    # 年齢と勤続年数の整合性
    if 'age' in df.columns and 'years_of_service' in df.columns:
        age_service_violations = df[df['age'] - df['years_of_service'] < 18]
        if len(age_service_violations) > 0:
            violations['age_service'] = age_service_violations
            print(f"年齢と勤続年数の整合性違反: {len(age_service_violations)}件")
    
    # 給与と年齢の整合性
    if 'age' in df.columns and 'salary' in df.columns:
        salary_age_violations = df[(df['age'] < 25) & (df['salary'] > 500000)]
        if len(salary_age_violations) > 0:
            violations['salary_age'] = salary_age_violations
            print(f"給与と年齢の整合性違反: {len(salary_age_violations)}件")
    
    return violations

# 論理的整合性チェックの実行
logical_violations = check_logical_consistency(df_cleaned)
```

#### 6.2 データ品質レポートの生成

```python
print("\n=== データ品質レポートの生成 ===")

def generate_data_quality_report(df):
    """データ品質レポートの生成"""
    report = {
        '基本情報': {
            '総行数': len(df),
            '総列数': len(df.columns),
            'メモリ使用量': f"{df.memory_usage(deep=True).sum() / 1024:.2f} KB"
        },
        '欠損値': {
            '欠損値の総数': df.isnull().sum().sum(),
            '欠損値を含む行数': df.isnull().any(axis=1).sum(),
            '欠損率': f"{df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100:.2f}%"
        },
        '重複データ': {
            '完全重複行数': df.duplicated().sum(),
            '重複率': f"{df.duplicated().sum() / len(df) * 100:.2f}%"
        },
        'データ型': {
            '数値型列数': len(df.select_dtypes(include=[np.number]).columns),
            'カテゴリ型列数': len(df.select_dtypes(include=['category']).columns),
            'オブジェクト型列数': len(df.select_dtypes(include=['object']).columns)
        }
    }
    
    return report

# データ品質レポートの生成
quality_report = generate_data_quality_report(df_cleaned)

print("データ品質レポート:")
for category, items in quality_report.items():
    print(f"\n{category}:")
    for key, value in items.items():
        print(f"  {key}: {value}")
```

### 演習7：包括的なデータクレンジングワークフロー

#### 7.1 統合的なクレンジングプロセス

```python
print("=== 包括的なデータクレンジングワークフロー ===")

def comprehensive_data_cleaning(df):
    """
    包括的なデータクレンジングプロセス
    """
    print("データクレンジングプロセス開始...")
    
    # 処理前のデータをコピー
    df_cleaned = df.copy()
    original_shape = df_cleaned.shape
    
    # 1. データ型の変換
    print("1. データ型の変換...")
    numeric_columns = ['age', 'salary', 'performance_score', 'years_of_service']
    for col in numeric_columns:
        if col in df_cleaned.columns:
            df_cleaned = clean_numeric_string(df_cleaned, col)
    
    # 2. 欠損値の処理
    print("2. 欠損値の処理...")
    # 数値変数は中央値で補完
    for col in numeric_columns:
        if col in df_cleaned.columns and df_cleaned[col].isnull().sum() > 0:
            median_value = df_cleaned[col].median()
            df_cleaned[col].fillna(median_value, inplace=True)
    
    # カテゴリ変数は最頻値で補完
    categorical_columns = df_cleaned.select_dtypes(include=['object']).columns
    for col in categorical_columns:
        if df_cleaned[col].isnull().sum() > 0:
            mode_value = df_cleaned[col].mode()[0]
            df_cleaned[col].fillna(mode_value, inplace=True)
    
    # 3. 外れ値の処理
    print("3. 外れ値の処理...")
    for col in numeric_columns:
        if col in df_cleaned.columns:
            df_cleaned, _, _ = cap_outliers(df_cleaned, col, 'iqr')
    
    # 4. 重複データの削除
    print("4. 重複データの削除...")
    df_cleaned = df_cleaned.drop_duplicates()
    
    # 5. データ型の最適化
    print("5. データ型の最適化...")
    for col in categorical_columns:
        if df_cleaned[col].nunique() < df_cleaned[col].count() * 0.5:
            df_cleaned[col] = df_cleaned[col].astype('category')
    
    # 6. データの一貫性チェック
    print("6. データの一貫性チェック...")
    range_violations = check_data_ranges(df_cleaned)
    logical_violations = check_logical_consistency(df_cleaned)
    
    # 処理結果の報告
    final_shape = df_cleaned.shape
    print(f"\nクレンジング完了:")
    print(f"  元データ形状: {original_shape}")
    print(f"  処理後データ形状: {final_shape}")
    print(f"  削除された行数: {original_shape[0] - final_shape[0]}")
    
    return df_cleaned

# 包括的なクレンジングの実行
df_final = comprehensive_data_cleaning(df)
```

#### 7.2 クレンジング結果の評価

```python
print("\n=== クレンジング結果の評価 ===")

# 処理前後の比較
print("処理前後の比較:")

# 基本統計の比較
print("\n年齢の統計比較:")
print("処理前:")
print(df['age'].describe())
print("\n処理後:")
print(df_final['age'].describe())

print("\n給与の統計比較:")
print("処理前:")
print(df['salary'].describe())
print("\n処理後:")
print(df_final['salary'].describe())

# データ品質の改善確認
print("\nデータ品質の改善:")
print(f"欠損値の削減: {df.isnull().sum().sum()} → {df_final.isnull().sum().sum()}")
print(f"重複データの削減: {df.duplicated().sum()} → {df_final.duplicated().sum()}")

# 最終的なデータ品質レポート
final_quality_report = generate_data_quality_report(df_final)
print("\n最終データ品質レポート:")
for category, items in final_quality_report.items():
    print(f"\n{category}:")
    for key, value in items.items():
        print(f"  {key}: {value}")
```

### 演習8：クレンジング済みデータの保存

#### 8.1 処理済みデータの保存

```python
print("=== クレンジング済みデータの保存 ===")

# クレンジング済みデータの保存
df_final.to_csv('cleaned_employee_data.csv', index=False, encoding='utf-8')
print("クレンジング済みデータを保存しました: cleaned_employee_data.csv")

# 処理ログの保存
processing_log = {
    '元データ行数': len(df),
    '処理後データ行数': len(df_final),
    '削除された行数': len(df) - len(df_final),
    '処理日時': pd.Timestamp.now(),
    '処理内容': [
        'データ型の変換',
        '欠損値の補完',
        '外れ値の処理',
        '重複データの削除',
        'データ型の最適化'
    ]
}

# ログをDataFrameとして保存
log_df = pd.DataFrame([processing_log])
log_df.to_csv('processing_log.csv', index=False, encoding='utf-8')
print("処理ログを保存しました: processing_log.csv")

# 統計サマリーの保存
summary_stats = df_final.describe()
summary_stats.to_csv('data_summary.csv', encoding='utf-8')
print("統計サマリーを保存しました: data_summary.csv")
```

### 演習のまとめ

この演習を通じて、以下のスキルを身につけました：

1. **データの探索と問題の特定**
   - 欠損値、外れ値、重複データの検出
   - データ品質の可視化
   - 問題の体系的な分析

2. **欠損値の処理**
   - 統計的手法による補完
   - グループ別補完
   - 適切な処理方法の選択

3. **外れ値の検出と処理**
   - IQR法とZ-score法による検出
   - 外れ値の制限処理
   - 可視化による確認

4. **データ型の変換と正規化**
   - 文字列データの数値変換
   - カテゴリ変数の処理
   - 標準化と正規化の実装

5. **重複データの処理**
   - 完全重複と部分重複の検出
   - 適切な重複削除戦略

6. **データの一貫性チェック**
   - 範囲チェックと論理チェック
   - データ品質レポートの生成

7. **包括的なワークフロー**
   - 統合的なクレンジングプロセス
   - 処理結果の評価
   - データの保存とログ管理

これらのスキルは、実際のデータ分析プロジェクトにおいて、信頼性の高いデータを準備するための重要な基盤となります。適切なデータクレンジングにより、分析の精度と信頼性を大幅に向上させることができます。

